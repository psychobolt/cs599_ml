{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.layers import Dense, Dropout, convolutional, normalization, Activation, Dropout, Flatten, pooling, Input\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.regularizers import l2\n",
    "from keras.initializers import VarianceScaling\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications import VGG16\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from scipy import linalg\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I'd consider putting your datafiles in a ancenstor directory outside this file\n",
    "\n",
    "train_images = pd.read_csv('../Hw2_data/train_data.csv', header=None).values.astype('float32')\n",
    "train_labels = pd.read_csv('../Hw2_data/train_target.csv', header=None).values.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ZCA whitening http://ufldl.stanford.edu/wiki/index.php/Implementing_PCA/Whitening\n",
    "def whiten(image):\n",
    "    # normalize\n",
    "    image = image - np.mean(image) / np.std(image)\n",
    "\n",
    "    # feature-wise center\n",
    "    image = image - np.mean(image)\n",
    "\n",
    "    # feature-wise std normalization\n",
    "    image /= (np.std(image) + K.epsilon())\n",
    "\n",
    "    sigma = np.dot(image.T, image) / image.shape[0]\n",
    "    U, S, V = linalg.svd(sigma) # U = eigen values, S = eigen vector\n",
    "    epsilon = 0.1 # 10e-7 is keras' default\n",
    "    principal_components = np.dot(np.diag(1. / np.sqrt(S + epsilon)), U.T)\n",
    "    \n",
    "    return np.dot(image, np.dot(U, principal_components))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "orig_images = train_images\n",
    "\n",
    "# Preprocess images\n",
    "train_images = whiten(train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16175, 2304) (16175, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAACRCAYAAADTnUPWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJztnXuwnVWZ5p91TpKTkAgkIBi5hZtCuEvkpjS00LaIIFo2\nJd5g1NKq1ul27JpunZmqmbGna7rn0t1OdfeUjLZKaxdiayu2WKhAVNQGI0QuCZeAiNwMl0BCwsnJ\nyVnzx96//a397G+ds0+yzz4nO+upOrXP3t9tfetd37ee913vJcQYVVBQUFCw52NothtQUFBQUNAb\nlBd6QUFBwYCgvNALCgoKBgTlhV5QUFAwICgv9IKCgoIBQXmhFxQUFAwIygu9oKCgYECw17zQQwiP\nhBAutN+uCiHcOlttKth9FLnuHQghrA4hbAohjMx2W+Yy9poXekFBwZ6JEMIKSedKipIunaFrzJuJ\n8/Yb5YXeRAjhEyGEh0IIW0II60IIb0u2XRVC+HEI4W9CCC+EEO4LIVyQbF8dQvjvIYTbQwibQwjf\nDCEsa277dgjh39q17krPXzBzKHIdCLxP0r9K+oKkK/kxhPCFEMLfNmWxJYRwWwjh6GT7G0MI9zdl\n+3chhB+EED7Y3Ibs/yqE8KykT4UQngshnJQcf1AIYVsI4eV9u9PdRHmhV3hIDRawn6T/KulLIYTl\nyfYzm/scKOk/S/o6D3cT75P0fknLJY1L+j/N378o6T3sFEI4RdIhkr49M7dRYChy3fPxPklfbv79\nbgjh4GTbO9WQ61JJGyT9mSSFEA6U9E+SPinpAEn3SzrHznumpIclHSzpTyVdq0Smkq6QdFOM8eke\n38/MIca4V/xJekTSi5KeT/62Sbo1s/9aSW9t/n+VpCckhWT77ZLe2/x/taQ/T7atlDQmaVjSQkmb\nJB3b3Pa/JP3dbPfHoPwVuQ72n6TXS9oh6cDm9/sk/bvm/1+Q9Nlk3zdLuq/5//sk/TTZFiT9WtIH\nE9k/atc6U9KjjAdJayRdPtt9MJ2/vY2hXxZj3J8/Sb/PhhDC+0IIa0MIz4cQnpd0ohqsDTwem1Ju\n4leSXpl8/7Vtm6/GIByV9BVJ7wkhDKkx6/9Db29rr0eR6+DiSknfjTE+0/z+j0rMLpKeSv7fJmlJ\n8/9XKpFdU8aP2blT2SrGeFvzHOeHEI6TdIyk63f3BvqJgVgI2F2EEI6Q9P8kXaDGrL4zhLBWjVkd\nHBJCCMnDf7jahX1Y8v/harAKBuEX1XjYb5W0Lcb40xm4jQJDkeuejRDCIkmXSxoOIfDiHpG0f9PE\nNRmelHRocq6Qfm+iLtUsprSnJP1Tc+LeY7C3MfQcFqsh3KclKYTwb9RgcikOkvQHIYT5IYTfk3S8\npBuS7e8JIawMIewj6VNqDIadktR80Cck/W8VFtdPFLnu2bhM0k41TF2nNv+Ol/QjNUwqk+Hbkk4K\nIVzW9GD5iKRXdHHNL0l6mxov9Wt2sd2zhvJClxRjXKfGQ/lTSb+RdJKkH9tut0k6Vg129meS3hFj\nfDbZ/g9q2PSeUsO++gd2/DXN836px80vyKDIdY/HlZI+H2N8NMb4FH+S/kbSuzWJhaFpovk9Sf9D\n0rNqTAprJG2f7IIxxl9LukMNIvCjntxFHxHazYcFdQghXKXGYsrrM9tXS/pSjPGzk5zjfZI+lDtH\nQf9R5Lr3oLnO8Zikd8cYb5li37+X9ESM8T/1pXE9RLGh9wFNdf33Jf3dbLeloHcocp3bCCH8rhoa\n2EuS/r0aayf/OsUxKyS9XdJpM9y8GUExucwwmoPqaTVU/n+c5eYU9AhFrnsEzlYjxuAZSZeo4Q31\nUm7nEMKfSrpH0v+MMf6yP03sLXbL5BJCeJOkT6vhl/vZGOOf96phBbOHItfBRZHtYGOXX+ghhGFJ\nD0j6HTVsUz+TdEVzIapgD0WR6+CiyHbwsTs29DMkbYgxPixJIYRrJb1VUnZwLF26NB5yyCGt77nJ\nxH9/6aWGljRvXtXchlupNDTUsBrNnz9fkjQ+Pi5JeuaZhqvw9u2NRe2XvexlkqRFixbVHs93ru2f\ndW2bmJiQJO3cubPtkzawnc/c/Y2MNBLIDQ8P1/7OvdHGrVu3tvah/YA+4tjnnnuurU2StGXLFr30\n0ktB9Zi2XEMIMW3HggULJFV9zXfuj/sA6X27PPxzqu3+mbSx7TMn3xRTjYGpxgpyz+3n4yMdJzt2\n7JAkvfjii5KksbGxjn2kqj+Gh4c1Pj6uiYmJnFylacr2wAMPjCtWrNCTTz4pSdq8eXNb+xnvfp8g\nlcFU+7i8APszlvbbbz9J1fhmbOWOnw5ycs7JEZmMjjZc1XlP8c5hv/TZ4H8fx4A+9d+3bdv2TIxx\nypwyu/NCP0TtkVaPqRE624YQwockfUiSli9fruuuu661zQcnYDBzc+vXr5ckHXDAAa19eAnwoj7o\noIMkSU8/3Ui78PnPf16StGHDBknShRc2Mqwef/zxkqSFCxdKqgYGL0JefN4Gf3lLjRejVL1gGfC0\ngYcRQfsA4VrHHHOMJGn//feXVAn7iCOOkCS94hUN91le7LfffnurDfvss49SHHhgIwjy6KMbOYro\n76eeqgLqvva1r2kSTFuuIQQtXLiw1YevfGUj0PLkk09uu499991XUtXXyHDJkiWt83I/PKh8cu98\nd7mx3T99wue7P5QpfJvLi0/GAmPFH2S++3b/zvhgPEnSb37zG0nST37yE0nSo48+KqkaU9wH/bVk\nyZLWuJsEU8o2levhhx+uNWvW6FOf+pQk6Xvf+15bfzDe+fRnBFml95g+P1Ilv9xEzf48txdffLEk\n6cgjj5RUjS3kC5zo1MFf4E7IkBMvbJfb448/Lkm67777JEn33HOPJOnBBx+UVPUH7xqpGuuLFy+W\n1Eni6Ev6hXPccccdv5ryhtQHL5cY49WSrpakE044IdIpUn52QhiPPdaI1OWmmaWlqkN4gdHpP/zh\nD9uOfeMb3yhJOuWURmCZd5Q/rAjLQdu2bduW3pukSmC0m4HMi562AV4U3L8LlXtDuExkzz7bcI/m\nRZne72mnnda2D+c855xGPqIbbrih7T53F6lch4eHY3o/DFr6xe8PGfCZjgNn0N6nKSNN9+e++N0n\nz3TcSdVYSrU+wLlcM/Ix4vflLyF/OdEGjvN7SsceL2om840bN0qqXoy0hbHUK/fjVK6rVq2KO3fu\n1C9/2b4+SLu5Nu3n/tievryd3XIM9+HPAt9PP/10SdIll1wiqSI/MPX0nSDlSWJ67dy+OUbOOOB+\n+I5s/HnmuecdlD5zfk0fQzwz3rfdYne8XB5Xe1j0oc3fCvZsFLkOLopsBxy7w9B/JunYEMKRagyK\nd0p612QH7Nixo031Z/bJseUXXnhBkvTylzdMR6lahVrPrHn33XdLku69915J0kUXXSSpYrM5bcDb\nwgzpdkJnX1I1izIj587pZgKYmJsBYGVoAbRl06ZNbd9h4ZK0YsUKSZVKDqNZs2aNJOnss8+WJLF2\n8Ytf/KKjnYZpy5VMb2gWS5cubbtvWAj36wzVz5Xuw6ezP+874EwPBu7ndaaUsmP2QV4+Lv2cbt93\nk8xk95siNZ8xBpDbr3/dsJTA0Nme2tqnkKs0Tdnu2LFDTz/9dMv8Q3+4GRJ4n9NWqbOvplrjYNxe\ndtllkirTCn3krNnPM1lf5NY0OMa1CO6T++O5py3LljWyLSMTzEFukk334RyYWdE4eMbR6tNju8Eu\nv9BjjOMhhI9KulENF6i/jzHeu6vnK5gbKHIdXBTZDj52y4YeY7xB7YmMJsWzzz6rL3/5yx1M1RdE\nYHhnntlYr2E2ZiZMwcLEbbfdJkk69dRTJVU2ZT83s6zP4FyD2ZgZ0pniZDYtX+V2RsbszDmYpblf\nmA2sixmexa5jjz227Z4l6cQTG7mmWLA66qijJFUzP8z9pJMahVi+9a1v1S4EppiuXAH3gV0TBoO8\nfYWfz9SO7WyebTlmzndYFdecSu7eBrexp78xJvxabOcaPoaAa2o5ZpiOF/oM5nbYYQ1LCdoZ45Nz\njI+Pd2VHn45st2/frocffrg1/nj+YI30KRoJfc/2Os8T90bzvjv33HMlSW9961slVZo42p8vftLX\nnCfnWZbu47byqTykgGtuyB8Zcd8uK57BunbzLD7//POSKoaOAwHX6hYlUrSgoKBgQNDXXC4vvvii\nfvCDH7SYCLO0u6i97W2NsozM+G6bkyr/amzmzGSrVq1qO9ZnbF+pTl2KpE7mxyxcx2qdTQL3agAw\nd87F/fAdF0z6BYZOm7GjptfDLRO73c033yxJuuKKKyRJd955p6SGC5rUcP9at663cSTDw8NavHhx\ni6nALnJ+6M6+U2bpbDbnEeLeLJzL7dZ1ba1DOrbclurXcC8I5O3eDoxB5I5c3SsKpOPFYxPwqMCr\nC/dFd7XrJUZHR7Vu3bqWZsn4pJ3u5eVrDClLdg3J10ZwK7788sslVffpnka5dbddiS9wJu4urWx3\nl2bXLLkH2sx31/CkylvJNcScW62/n6ZCYegFBQUFA4K+MvQlS5bovPPO62DcbkOEjcBsmOHSFV/+\nx4kfmzkswn1IgdtDfcb3WRim5HY3qdNei1cO8NVyrul+6TBx2u6eGDAF92SRqqAG7h9GTpADffrQ\nQw9Jks466yzdeOON6iWGhoa0ePHiVn+4jzftpy0eDJTaCX0f90t2lp9jarBl98xwrcA9UqRqLcSv\nmWNozo5pm7eB3xm7PqZSrxC3LbMmsnx5o7410ZuMnRBCz3zRwUsvvaR77723dX8wda7jmra3PdVA\nnEHzbJ933nmSpCuvbFSV84Ab12JcO/I1EWfodTEOue9+TN0zX7efr40gKzzQ0rHF/6xL5DRG5Dpd\nmRaGXlBQUDAg6CtDX7Zsma644ooOGxssBLbtuVFgOulM59FreHcAZk2fXT3kH7iNHQbh3jDpce5J\n4TZyvw9ndhwHY2fWxu+e/TmeyNFUEyCKlLUE/HVJD/C6171OUsUKh4aGuvFXnhaGhoa0ZMmSjnB8\n7x9n3+yf9mmd/TX97szMPU98e06ObidPkfPA8DHikaP+nXO795OzSVha6sXFtfB+oA0HH3ywpGqd\nAtY8WYTkrmJ0dFT3339/6zttIRLY2aWnTEjZNX3DWD///PMlSR/72MckdXqE1bH89BrA7zsXaVy3\nTy7NgJ8D+PpNzvsJDRWbenpvnJvnlWeeY/0ZYq2kWxSGXlBQUDAg6CtDX7BggQ477LCOmQ57kduS\ngTNdSS1PDexUzhZ8FRl4kicH54FVMUNyXJrLxdmBR0LmbObMwp7sCQbE/Tvz57jUDx2fV7QbGDpt\nY9/UB77XbG5oaEgjIyMd3hu+juHaEZ/pSr73nbOnurwnUqdngdu9uedcZGkapVmX30Wq+pBPPK2I\nfn7iiSckVfKGcbP24eMbpkskcJqXxGMuGK8cg0cRvs65HES7g/HxcT377LPZfDjEHfCscN+0NX1W\nkAPeLB/96EclVTbz3JjMMXJf3+gmA2Yus2rORj7V2Mm9W9CeOI7EgVIlV+RIRDfRuIxzxkiakLAb\nFIZeUFBQMCDoK0MfGRnR0Ucf3eFbTMZAZnpsT54zIs1hwv9Elrl/q+dfmCoy0D1R3O+5Dm6ncx9q\n4N+dVfAdOzef7lVB22DyUmVThVXCyGEAsEZYwosvvthzhh5C0IIFCzp8/2kTLIPt2FhhuClDhz3B\nZj21LKye+4MlwnxgrvQZrJHxwbVgw5wvjTPgmrSX74888oikSj60/+c//7mkKk6A+0WuMFXWQND6\nuAePpE3BGEF+eIA5+58q+ndXEGPU2NhYRzpZPul72sj98Gym4+ztb3+7pMrP3L2P/BmaKsf8VPUL\nuvEO8XWZXJrkqfzVgWt2dTl80Ei4f5j4TTfdJEn61a8aWXLR+gpDLygoKNhLUV7oBQUFBQOCvppc\nYozasWNHS2VDdX344YcldZpL3DWN4BipUq0xz7j5w4/1RbJcGa1c+k3UZl/gTM/lC3q50Ohc+lFf\nBKV/OJ7P1JUJU4SbY1B/KcXHQs3mzZt77rY4PDys/fbbr2OhimsSBIMqzgIQ39OAMVeD3XSCWsti\nMJWZSDNLMifu3xNo8d0XyFIzFu3x4hH8jlpM2gUKGSAXlyu/cx5MFbinYkZJKze5mYY+4xjGv4+1\nXiLGqNHR0Wwqae4LORPWjvyvuuqq1rkoNOMmEU9+N1X621xq25ypZTK3VA9Ky7XNx4qbbXNplOtK\nS/pCKSYYLzWIa/J0n9XC0AsKCgoGBH1l6OPj49q4cWOL6cCKWLhz10NPKP/AAw+0thHq7gEiHuLN\nLOqh/rmCF7nAFdqSMnRfTHH3Sj8HbYJ9ech0LvEUi3LM5jAiqVqYYxsucK6JsN/IyMi0y1p1g7Q/\nkSsLPbBL4G5tsND0PK7VeOInjoXl476KRoL7JqyXBc5cwEpduTQvsI02xOIn9+lJ5HyBEq2BTxay\naVtdCgF+g71zLOOPY/n0tBO9QIyRwtOSOp8RTy5HP3zwgx+UJL32ta9tncvdSYEvbuYYac591TGZ\na2GukIUjF47vWsFUqQTqfvf75bl99atfLalaePdawWvXrq09d0cbu9qroKCgoGDOo68MfWJiQlu3\nbm2xDJgO9iN3OWPmxxWvLjEWdkwAszv00EMlVWyImdALuToD57xuK68LB/YSU3zmAjHcdu7fuV9P\nr0tb6qqIw249AZTbnGGTRxxxRJZJ7CpwW4TRonEhT+zcXhgAuad97QnZ3AbuDIz9sWv78TB1D+bK\npXqQOt0L+Q57Qjtwrc2TzOFyhn3fUzoAXCjTNtCX3L8HYXkh7plCGojm45R+wYXywx/+sKSKmafj\nPxfanyvuPVWxCd/PNbg6Bu/Pm7sf5rS2nPbgGroz9zprgJ8LuWJTZ4zgtlhs6AUFBQV7KfrK0Hfu\n3KktW7a0Zvg0UEiqZi9PqA9DT5M4YTNkH2YyZjYYDiXpYDRuN8vZ7bFTc766AA4YupcD43cvKwU4\nBwzGg6I8UQ/3ChOqYz6+duCske+p10KvAcN81ateJaliqK5xANKMpl4u9J17htB+Txvg58SG7izM\nyxu611NduD/9ToIq1ic8IRZtRj6vec1rJFXagXsyMO4Z17QZdpb2DddEjr4mALsfHh7uufcSgUU+\nPrHn+xoCnkd19vKcbdvZrXupuVxydnxfa6nTXHJeZVMVkPfAKn9+ve1ecL6u0IePS95jjAEvJtIt\nCkMvKCgoGBDMig0dv2SYic+ybtckxeSll17aOhd2Sfb1xEewIBg2jMfLSrldDSaIfZA0tJwvnTH5\nn3On9ylV9mxsym5zyxWspo1oCR5KnTIfXxPIaRxg69atM5JqdWJiosXI6Q+3/3oiJthnuqIP+/Pk\nXCQfu+uuuyRVckLeXmzEizHAnulTzo+dP4XbtBl/qSYhVWOO1M2koeD+Gc8wc/qDdR0YLdofiZqk\niv362PI2pqlsZ2JtZN68ea12U6T8jDPOkCTdeuutkio/fNrM+lX6rExVoBnk0iaDXAyH2/dBncdK\nLnEfz3xaNETKpwLwVB++HleXZiRXzpBj8dZifZF3ZbcoDL2goKBgQNBXhj48PKx999235U/OzOUl\nnJgBsTFii6IQslSxO4+qZBaFDTLzpX7Y6TlzBV1hTZyX86UsDXsX2/iEPdEW/LBJak9xDvbzmT1X\nPg9WynXSfb2AAHD2PzY2NiMMPWVatMEjR4n0RWs65ZRTJFWsROrsU+TEWggeFKtXr5ZUleDjms6O\nYTonnniipKrP+azzXoJZeXpc96AgFgJf8Z/85CeSpDvuuENSJT/GKmOHgt3cNz7IJ5xwQqsNMDNP\n4OUxDawJ5FL+9gJc2++XfuC+6CdkVZfSN+fFMlV8QC6a02M5GGteTCTdxrPOmEGz4H2DRsY10Ti4\nb/rc31u5xGLpM+iJ61xboMQg6zBo99/4xjfUDQpDLygoKBgQ9D1S9Jlnnskm5XdfcGx0zFppRKH7\nZvvM7qlLmSWxoTK7wgy8vBuzts/4aREC/qcNHAtj8eIJ+GM7o3H7r3uo+NpCaif0Gd69N7w/xsbG\neu7lQoGLXPpQZ7pnn322pKr/SD8rVZoRx8C42Rc2e+aZZ0qq+pR4BNiVewcgm1wR6VTrYV/WBI4/\n/nhJnXlkYP8/+tGPJHWWFcuVMoPxoamyLgBzl6o1ItrJuIWxM65hi4sXL24rMt0LsOaF7fykk06S\nVI1f98DhueZ31sikzvKL9Aly8nxFnBO5+NhHU3GvF/+9zisN1kvsAvtwXxyL/Cm4/rOf/UxS9S5g\nvYcx6rLycoFSvgyje8YwRtJju0Fh6AUFBQUDgr4y9G3btmnt2rUtBsRMx2wFU4UtYx/F1pjah7F3\nMdsyw6WMJQWzJ8zVZ1XYBawL5kCb8KZIvQ6YTWFLMCRm+rQEl1QxeWxxMAb6wX1nc+W2UvbtkY/O\nzIFnsuwlQgiaP39+h12aa5GFD9aJXzcMNdW8kCdwf+W7775bkrRq1SpJFWOF0WBjdybu3k2eCS9d\nr0AOaIZoAYyBO++8U1Ilb7Yz9tavXy+p8m4hChB2BtNlPLNf6ocO+3M/eT45Fwxu3333bWk1vcKC\nBQt0+OGH67zzzmtrH/JknHveES+WLHVmu+TZQBNGq+GZQO70HeeCNeO1hHy9IE3dmhIaA884+5Kx\nlXcGzPr000+XVNmvWftCA0FzYewhd55nj1qWOj3bXMOgnxif3rdToTD0goKCggFB3yNFN2/e3Jo1\n3a+T2RvmxgzHDEguDUn6zne+I6kzcyH7wnCwfzLTsZ/nG8aeBvvwYq58pnZ/GIoXuQawB2ZoZl1Y\nFcwAbcNnaWcfXkA2bQN951qPHzvdyLNuMW/evI4+djs4/sp4cNAPqe2YfkceMBqPwoSJY3PnXLBk\ntDoYv/uQ0z/e1+lvnv+HPjznnHMkVV4pMD72YzvsErmjBXqGTMZW6g0Bm2Q8u/bDuejzpUuXtvq3\nV4Cho4EgC/qUZ4/79AyndaXXOIY+Qwvn07NtupdZqs1J1TPk8nSbtFT1Ff0N22cfxgCaJPudddZZ\nkqrnm/5gLNJGxqSXjUs9wGin57H3dTKQK2afQ2HoBQUFBQOCWYkUxWbIzMhsSjQgNipYFzNemvMZ\nX1fsnNg12QemxqzrK9e+Uo+vuLMMZnW2pwwBmxreG2geqS1UqmzmvhJPlCLH0fY070rads88KOVt\n4uwLm3ff8F6CiELPswK7oA3YS+l7tqfrEhdeeKGk6r6Qr3sx0XewIuTmXkqcx2MDfF0izRNEu5yh\ns4/LBW3Q12m4T7QIGJ9Hf6KxpPZeWC/2XMYM18B+nxbN7nWe+5GREb3qVa9qPYec3/20YcledSh9\nXln7wKOJZwQ7NPvyO1or+XBog0dGMz6QmUeG1nl0MR5h4mhMHMszjpbItS+66CJJFVPnufe1MV+v\nqcuH7n3lOW0ma/9kKAy9oKCgYEAwJUMPIRwm6RpJB0uKkq6OMX46hLBM0lckrZD0iKTLY4ybJjvX\nxMSEtm3b1lF1hdnY7ZzMzjCadMXX7VxEkbrtzVfiseHBeDynObZaZnH2h8ljP5MqeztsEHZBtkEi\nI2EAsAns3FzLGZ0zwNyKeHq/tMG9Ivx74vXRM7kODQ1pn3326cjVjTbE/bv/Lgw4ZcfIxTPcwWhg\ngxzjbJ9zetQefef1Qut8xT3fCOf2KFO20/e0nX5wDwXPd8/Y4p5geFLFCt1ryW3MqRdWM59Lz+Q6\nb948LVu2rMWekSMM3WsH4L1ETpu07bBZZH/BBRe02i1V2i7rZzzzPEvIm3cFMkBToe89ojRdM2If\nxgjtQ9vjO+fk+fUIbj8OWSGTybQE16KmYuAzwdDHJf1RjHGlpLMkfSSEsFLSJyTdFGM8VtJNze8F\new6KXAcTRa57MaZk6DHGJyU92fx/SwhhvaRDJL1V0vnN3b4oabWkP5nsXDB0WIbnuICp8PvKlSsl\n1TMetzV5HnBmdFiwR37mckO47zizMT6pqZ0Tey/b3FvHq+7AWN23mrYz4ztTB5wnjQjk2ByL4JNr\nzp8/XyGEnsp1aGhIixYtajFz1i+Qm+c9h605003/ZxtaDzZl9+11TxLWZ7wSjnsaeT71ulziHvnK\ntWF4aIkwNGzhaG5ez9ajXhnPtCVlY3U5eNL78sybiUbSM7nOnz9fBx10UKtdtPeSSy5puy/uh/GK\nhw4aqSS94Q1vkFR557AmxTle//rXS6q0FF+H8toCuVwo7imWgr7yNQy3Z9P3rF9w32hq3veeQdF/\nT+XqNnPXBv2Y6a55TcuGHkJYIek0SbdJOrg5eCTpKTVUvLpjPhRCWBNCWFOXrKdg9rG7cvVJsGBu\nYHfl6i6CBXMfXXu5hBCWSPqapI/FGDdbnbwYQqg19sQYr5Z0tSQtW7YsSp32URgPMxx2NXxw62Yr\nr+LuVbSZ+bkG7JAZ3qO5PMeEawCwD9inVLEm98KBQcMeuSZtYjts0m3jntMFBufVfKSK1btNmT6m\njfTp0NCQr7rvtlyXL18eh4eHO/JUpFqBVK0/eM3Y9Jqe88Nz7njELr/TJ+6Xz3a0B9i05wBK/X3Z\n1z0PvHoS9+U2cCY42pJjXc4u0zHHy9Tl6OyQz3nz5vVcrscdd1xcsWJFRz/Qt7QNhs598xykzJR2\n8ixgjyf/DeC59lq49LFr0p43v+Z+OtoAcnnRaaN7pfn49neP27tzUdtSpwbmfexaQ7foiqGHEOar\nMTi+HGP8evPn34QQlje3L5e0cVpXLph1FLkOJopc91504+USJH1O0voY418mm66XdKWkP29+frOb\nC042azPzwb4801pq03R7l+c6dpbkniJeF9Bnb8+vDTtO93MbsNtl3Y7t9t9cBXtnY7SZe4a1pO3z\nXBZ1lVrYL4TQc7kODQ112HPpB/cCYDv3m7Jj91bw/N+eP7suX0YKNCo0Gc9sWaf90U7kxlhyLY7f\nXY5e7xN4DVngY1Oq5Mo1nBV7ZOH4+LhijD2VK5Hdvg4D0DRh5Kwd8OykXjuuKXllKR/zjAmPlOR+\nPUshv/vgZ5fvAAAdg0lEQVT4SSOAp2LoXn9gMm0ovSfX6idj5q7t+b7+7Ey3Tmw3JpfXSXqvpLtD\nCGubv/0HNQbGdSGED0j6laTLp3XlgtlGketgosh1L0Y3Xi63SsoZci6Y7gXT6uSeydBzlfgslvor\ne04EZ+rOxH1B1vOkY7N0P2ZfyU9nTM7p0V+5WdhncmfqMBu367Pdbenp/zBQ7w+QVo0fGhrqqVyJ\nFPVMd36fzrppc6q1uR+6s1zvW/cs8khQz3+fY2Pp2HIm55oS49Ztp+5nDtxm7tHIddV6PBKUcZmr\nsAV6KVdn6L7Wg2x8nHoN1bp9eMZdvg5n4GhPzoZdewKT2aBz6yi5Wr9u185VWfJr1mmP3exT14ap\nUCJFCwoKCgYEfc3lEkLQ0NBQR5Y5r9dZ558stdvQmalzGfGwM3tOE7fX4xfL7IsPLcyO/YhwS2dW\n9wv3Gd6ZtzNwmAzHu48qTC+3Ci91MhS2ee4Wr384E8hVYnefcPdMSOVKO2GmLldn/64FMYZg//Sd\nZ1B0WaV+z/jJIyf3IKJvuTZjZSo25VXg+ayTia8ROCt0++2OHTt6LtsFCxa0PM2kqs8Zr64lub27\nLoOl26Hdw8eZq1/DPcXoQ7dr18mC33LrJx7p61HGzuhzNnTX3Ork4vc7Va3RblEYekFBQcGAoLzQ\nCwoKCgYEfTe5pMWEcSnDxIKK5u5tIF2s4hyYbVC5WSRkkdPTsHIcajXHc02OJ/CBBD0UTEjDhlG1\nUcUx86SBP1KlyqE+s5+nQnUVz9VGN6+k7Qa+cOOpbEdHR2ekDF2dOyr37YuMXgAhlbO7qPqCaq5k\nl5utcmaw3MIXLnfpsciLBXS+Y77DXY/7cdMLbfTQfu//uqIjaVHv9BN4AQcPGJtJcD+e1pk0wTfc\ncIMk6f3vf3/rGA+bzwVZAe8j78NcX/qzk/aJu5+6E4OnavD9gLcht0BdJ4+cWSx3/8XkUlBQULCX\noq8MfWhoSAsXLmyxCkLjPYDDgwNA6noIA4cNw5Y8ORMLWJTn8kVTjmMWphwarmmk6yQxPwUVUpAA\nzGdTT67lTJ1Pn7WZ8d0NsA5sQ9NIiwpInelWJzvXriLGqJ07d3awImfq3KcHatSF/tMHMFMvksx2\n7tfd39CwvC8pfeZ9nrIwT7WLNge8MAdJuRi/nn6Cc/sC52SLZiC3sAxSTbXXBS62b9+uDRs2dLj8\nerlDEm1RFpLvl156aesYT6cwVci+jxF3x3V3VR9bvnic7stz6aH8HgCYOydw+fkC70w6IORQGHpB\nQUHBgKDvDH3JkiUtZk4BC0o9wYqdTTKjpgzEWb0n38LmtmHDBkmVfQ/2hJ30rrvuajuehPrsh1uj\nf0pVon9+S+3UUj4wgU93wcuFWAP2S230/I/9FltwjlWMjIz0nMlJDTbizC1nJ53Mvc9dNn1dge+s\ndXgAjic+g6nznXWbXBGLtL1oZ7iykk6Va7v2wDnR/hjPzirdZuu23PR/d5nzxHZpuuFey/W5557T\ntdde2xrnXJv+cAbrWTfTxFup+6PUGarv5wK5FBbOfv14D+JKtzFG6DveGe4a6+6NubJx3obJNK/p\n2tBnosBFQUFBQcEegL4y9OHhYe233346/vjjJVXBHOvXr5dU2UFhmx6Qk85iOWbuCb24FufA9g6T\nJfXtq1/9akmdZcOOO+44SZ3sjPuRKqbuq/9c0+3BHlDjdm33wJgsLSd9hueFe+14oMJMekLkkpB5\n+/kdGab2b0/Z4Me61wd9DBOHfSGnRx55pK1tJ598sqSqn9ifcSNVBVawCTNmTjrpJEnVmKJPGTt4\nUjG2aAP9kQtbB3VszJNxeVDSTBb/luqfOfcY8qAe2rRmzZrWsWeffbakapw6u3Vm7VpdLsAM+Fir\nC8yh/z01AeswaAt17D6F28pzaZbBZEw9t57SzfpKbdumtXdBQUFBwZxFXxn6okWLtHLlypZPN7ZG\nGBD2a+yc2Kbr2FpuNvSwbFgEzJuUnjBvmIHbWnP+z6nPMMdyTbQFD/n3ogvORjyhlHuJuJ9yqkXg\nYQPLhYXAhDzJ1cKFC7N2yV3FxMSEtm/fnl0TcHYJ6jwR3GaO7PnuKWtZG8ETBe3uvvvukyTdcccd\nkjplsWrVqrbvqZ/3bbfdJkl6+OGH29pPOxkraJjHHHNM23bkw1iCsaMF5BKOpcilOOA+vI8pLdhr\nTExMtMYSfeU+4e6vj2xYv5KkdevWSZLOPPNMSZ3PgidLyyWvcnt2XRrhuuOlSivjfeNjzLX9nGfR\nVL7wvl+6ZjSVh1POx71bFIZeUFBQMCDoK0OfN2+eXvGKV7RmcmzPzPCwZ09eD7NJvTtgOe7rm1vp\nzxWIBT5b08Zc8qf0nMzozPw5O6d7RXjbPXGRz/wwv9SbgL7hvjgnfegr+AsXLuw5k8PDxYsU5OII\n3EaZ2kOdcbm9nT6GbWHvxl7r/s5og/Qt44axRH+l9TPZ59hjj5XUGYUK4/b0sbTRYwOwpaNx5tY3\nUk8b92Jxbc49NV72spf1XPMiLfIZZ5whqdKG8DOnvWi7tPH++++XJN16662tc/34xz+WVBV+Zyz7\n2E+vXffpmokzXlDnUcU1SdzmRWuQH1qveyf5p49R1xImWxPxZ7CuDKFUje9uURh6QUFBwYCgrwx9\nYmJCW7dubc1OzPR8h2U6O8FWWce+PZ8KLAg2yKwJk4WJ4d0C0BZg02735feUScBQ3G6dsvi0DdwP\nbZnKQ8FTbHKvqV0N/15nPLBDt8FRgq7XiDFmI2A9D4l7E9SlWeXe6SMYKWMEduuFmGHevoaCbNBc\nYNnILm0D12Kbew7RhlNOOaVtu2sgrmE6U/diG2leHvew8AhZwP3s2LGj53I94IAD9N73vrfVd76m\n4wVp+HzwwQcltfuh422ERxveRvS7e844nO2yn3uOTRbdiVx/+ctfSqrWQtDiPI2uF1sBOWbummkd\nU3cG7vmZXFMhwr1bFIZeUFBQMCDoO0PfsmVLa5aCFXtxBliXl4VL4ZkZiV67+eabJVVs6bzzzmvb\nzuycYwI5xucl7qSKkRx22GGSOr0d3CPBGR7+r7koMWZv2nDjjTdKqvygparwhucsoW3OBmcqK9/O\nnTs7ojzpS4/GdO0nvX9ngdy7rwlwDPZOZ2jO9GkLLK3OawlwjUMOOURS5RHDdzQl2uD2bL8/z973\nxBNPtB2HrLDtpv/7ugvwWIaZyKA5MjKio48+uoNp+nqSx4tgQ0+1ngMPPFCS9Itf/EJS1ZfEd3iu\nnlwJRY/KdR9yxk+dFwn9zifjkfvgWcqVlPOx4m10zcU9ldJz56KHPTqV/uoWhaEXFBQUDAj6ztC3\nb9/eYs/MjO7Pja0RBl8X9cVMDXPDlswq+u233962/YIL2uvjYj9jFRk7PXllPMf5Aw880Pa7VDER\nctNwDreVMhtjt8+xkLSf0n6hrdghL7zwwta+aC/0lXvKeJ6QsbGxnrM5si162TSPLATOolL2ybGp\n14nU6eHDPSBvzonNkf6AwXHeXLbJtA14W73pTW+SVLFLt9cCxphHNDszJ3cRjBz2hVxhrVLlQcMn\n45R+4NypLbnXcp2YmNDo6GhrPHPfyIYsk2zHdv7DH/5QUvv45tnAG4l2v+td72q1P/09N0ZcS+Aa\naOC5PFBS9SwT8UvfIq9c7h2PWqWtrol5ZHfO2yvd17UB5EpbyfLaLQpDLygoKBgQ9JWhS42ZiRnd\nZyfYF7MXDACGChNK4WwJn1hW06+77jpJVfQpvsXY7rDBMrPTBpg63hC0Jc03jm2Va/rquK/gw8y4\nf2eLudV0ZmvuAaaXtpfMgMC1gDTD30zkaaYAuFSxYe7T1ys8KjBlctjMYaTIxRmpF5M+66yz2q7N\neegX5IjdGpmwX6pF/PZv/7Yk6cQTT5TU6QNP/7kXhFePQv4el8BxnJ/vjDGpkrFXQcoVKp4JjI2N\n6ZFHHmkx2W984xuSKtbLM4acfbzyjHEuqdKcbrnlFklVjhfWn9LIVymfJ4VP1qM84tczf6btQVvw\n5yDn6w68GpLnbMrlXqo7h0ehesQsmgxrft2iMPSCgoKCAUFfGTq2Vs8J4fVAmSGxQcOyyPUida44\n+/fTTjtNUlWh5u677247l/ulMzuzAu7+59hViZqTqupGbiOm/dwP12I/ZzT+6VV7YDx406TRrtgO\n2Yd2OlMBM8HQQwhtuUQ86s0rNwH3JZeq+2FMwKbcy8U9D5zpYPcmA6KvLbCeUZePw3OVeN4Qj1UA\nHvvg29EGYPycBz92tMW69qKR+FiZyeo4ixYt0sknn9y69sc//nFJ1bMFm0RGztDRSKUq5gSg6d50\n002S2tcPpM7o2ZwXE23wLI5sT2Xgvu6+5uMM3fsYsL/HrqBNMUbrao26d5U/Kzy33/3udzva3w0K\nQy8oKCgYEPTdy2V0dLQ1wzG7OoPzDHtEnB111FHZfTw7GbMlUWBu32a2hqm53QwWDdNj/9RX2Osc\nwjr49KosMBdneGmOmrQtfm+szqe2Wq+ek2PHML3R0dGeszlyueQyyeWi+OpqptLuI488UlLlieDe\nOtjCnbkC+tRjATgf2h+ySTNY0k5s2jAxz8GPPD1rqK8hcH/O4E4//XRJlf8z26VqnDGe3ac5lSfb\ney3XkZERHXnkkS1vHPqKNSM0CvoaLxeeOWQoVTZzZI4877zzTknVGgi55d0fOxc34msIPk5S7yXX\nxhgzjCW/pmuBaI8eZYyG4u8Y16akfB1h7u+ee+6RJK1du1ZSpyVhKhSGXlBQUDAg6LsNfWxsrMU2\nsFX6LMuMiG85vsWpHQ67o7P7XH4JrzKem/ndp9jrP9b5TPsqt/tKO7vyajsOmAIMD+8WPAFS4A3C\nOTnGo/fSVfRe+yuPjY3p0UcfbTFN9xV2rxD3sYbhSpUnAnJwbccz3cFcYE+wRa9s5PlY/Hs6Dugz\nzuV5grim9yO/w/gYt7QNOV522WWSpHPPPVdSpQmk3j+eTdL9tD3icCY0L2oA43HBs4S9++ijj5ZU\nPQevfe1rJVVjbfXq1a1z0Tf0N/sge/ZlDHn2UzQWZ+C5fPHIoK5+gT8j7MMxfi7253fayPPodW49\ni2j6nHM/MG/XAr71rW9JqvqUcdAtCkMvKCgoGBD03Q99YmKiNRvlamt6hkRYChFoUhXFB3vy3NT+\niWcIzM/9tP2azLYwizq/X2Zb923me84/FebHfs70YTG0gWhIbJZpjozULz49p1dgTz0wes3kdu7c\nqeeff76jYo9H0LkXDGwzZSEcQwUit1PDdhkT9BF9RtQu/YJM0O6w78I2YdHpugSeQqyvcK5zzjmn\n7b7T9RRJ+ulPfyqpsrESy8A6DMz8He94h6ROzS2tZOTaXc5Dg/7aunXrjORzmZiYaFV9gpGjadAm\nvnsVKWzqUmeVJl9fwmaMfR5vMmfcrnHzLDFeeB94tSypegY8D5S3kXcAfct+yBsNxXNS+fqV54VK\n2+/vJ9YS+PSo1W5RGHpBQUHBgKBrhh5CGJa0RtLjMca3hBCOlHStpAMk/VzSe2OMY5OdQ2rMYr7S\nDbxaPCwEBgBTkKpIOnzT3XPG8zMwy+bs9r4y7VWF6mZKZ+buy84xXBPWDNtkBud32BeshNn8hBNO\naOuXlKF7PmXYZM6WvHjx4jbG0Au5jo+P65lnnmkxGu6XvqOfnHXQb7DqtC+QJ/ED3//+99vui9qU\n+ObD7LBv0g8wvbvuuktSFZ1Jxko8OFI7Phk60erwsmKsIR8imGkrDO7666+XVHklXXTRRW1t9ipJ\nrtFInV5bOWZOWzZv3uy1K3dbrqOjo3rwwQdbeYzwREn9y9P7YXzC0FONAfbLmPe1LO6HjKLEeCBP\nxgV9znhGY6Mv3RsN7UjqfPbdIuD5fnwcMx48ytg1LF+fSyPc0/z1UqVh4gXkkcAzydD/UNL65Ptf\nSPqrGOMxkjZJ+sC0rlwwV1DkOpgoct0L0RVDDyEcKuliSX8m6eOhMcW9QdK7mrt8UdJ/kfR/uzmf\nR97x3W3NPpulngj//M//LKnK1obdEzhrcHu2+0B7hW5vm+d6SdsFsJ3CImAJXBtvBtigV9eBZbMd\nWx0so46hA/qKa3Nuj140FtcTuY6Pj2vTpk0t9uXVkrxOK4yU+0YDkyoZ03deiQkmw33BtM8///y2\na9NX5O7BJ5rtXJPMlXX1arkmaxhpXhCp6nOPQ4BVoh2kuXfSfvBas6nmxFhgG+11Txq+P/fcc2m9\nyp7IdcuWLbr55ptbzxLPmHva0E/cP+MXzVKqmLdrxsiJvuQZwdvjyiuvlFTJzWM6eDYYL56bnmdS\nqvqQbf78Ae6LccA1YOjuz47G4mPPLQ1S9UxwDjyIuG+360+3Tmy3DP2vJf2xJHSoAyQ9H2NkxeEx\nSYfUHRhC+FAIYU0IYY0H0BTMOnoi15kIOy/YLZTndS/FlC/0EMJbJG2MMU4vMW8TMcarY4yrYoyr\nputTWTBz6KVcZzLjX8H0UJ7XvRvdmFxeJ+nSEMKbJS2UtK+kT0vaP4QwrznrHyrp8UnO0cLExERH\nibZcwVcPLkmTUuES9ZWvfEWSdOmll0qqFlPchcvTU3rqU0+c5KlvvSRa2n7clgg8Qf3DBMHCDeYQ\nD/334s+o2yz0oYbVuSRyDnfJ8gWemhJ6PZMrbov0javmHq6PPHELTF8c7sJKH2BaYcHymmuukSQ9\n9NBDkqTf+q3fklTJlbSjmD88oIW2epGDtL2APsW8gRpP+li2kwgOEw3mIsazywaZ+IKn1Jl6mE/6\n0FNUbNy4kd96JtfR0VFt2LChJQPMVp66wdtMwQ6ei3TfXDIxL9OI+x7J1TCp8U7AfOJmMO9j+kuq\nxp0XIvGEbbgHu4uhm1poiwcn5koRpv+vW7dOkrRhw4a2awB38e0WU+4dY/xkjPHQGOMKSe+UdHOM\n8d2SbpH0juZuV0r65rSuXDCrKHIdTBS57t3YncCiP5F0bQjhv0m6U9LnpjqAJE6+COqzkAfx+MKk\n1FlomTScJMw/9dRT264Bc/OgAS+cAPv1oBgYHWw7/R/mDSNjX1/89Pv1gscwQLQMUh+kLnVSeyix\npyD2hWWYCgygi/S505Yr16U/vFi2l4/zUPo06AR5uDsbfUTqVljUd77zHUnSZz7zGUnVQhyuozAh\n+hSWiRZBm2lL2h7YHqUGCRRydsi5WIB119kc4/fAutS9jXaxIOfh6gRWpWNwilSr05brPvvso1NP\nPbWj2Lc/v1zXU+TiUipVTNRLywFPiMV+lGAjqIlz+juBfmGsoSWnzJf7QG488xwDI89pID4WpzI1\nevoCqdIo77vvvrb2T5UgrFtM64UeY1wtaXXz/4clnTHZ/gV7BopcBxNFrnsf+p6cK8bYwcK6nYVS\nhu6zJ7MqASgwceyZHgTkLmM5l0CYGzbZ1C7opfS8VJm7NXoBW9gD10brIK2qMz13sZTUsR7htsM6\n9jdTYF0jLQKSXpv7p43YolOG7oW03c2LPiaJF26rhH4jP86DdoTcYEi0CYabFpdAHl54grQByMFT\nOAMvYuA2Ve7Fw+Hr3FHpMwqzIGf6gT7ctGnTtINQpsK8efO0dOnS1rPEmOKT8Yk8aSMBVGg0abs5\nVy61MizZy7vdcMMNkqSLL75YUntqXqnqJ+/b1L6dC/xxjcMLrCfuoLWfDl/XIr2EVGl7rJux7uJr\nAbuKEvpfUFBQMCDoK0P3UmXuaTBVIdzUHsZMj/06V7bNi07AzDzZEdd2+y6MDgaYsm7XEtyGCWtw\nZs41+R1Gf/LJJ0uqbLLM3n5+L7Kctss1D/egWLhw4bRXzrsFDJ2+IjDKk7G51wf2YKmSK3Jyuy1g\n7NAn2LU9ZSlMnb6GGXE+7KdpgQuYuYeEE9IO46L9MPhcAipPTubeP8iDsZyei/tEG2R80mdojjOR\ndE1qPHMeKu/BOy5H2g4blTrXRjyYztNe+zkJvPnqV78qSfrABxqBrmhq7rHiz3e6T86DJGc7d/nl\nSk7SP2hPPAe8Q6TKts852cdD/b2fukVh6AUFBQUDgr4ydPyVYUmwsNxM6Uw0Zei+Iu3HepFhGDt2\nPhi7hy/Dlpk5vfRT2ia3WXphX28jzAYGA/ODycLQYZdcixnf/ZbTfd2umyvckWpIvcTQ0FDrfvAh\nxrPEy+O5lpT69sPIPNQ7Bw+zdi0JZgv7pS+9QEq6NuLME20NdowcYOZoFXUh/FKnj7GnpaCNfEqV\nlsY49H3QEtLiGHU2+F7A0x3Tx/QHzwjaEPbtVOOgD7y4s1/D14Lc4wjvJVJ/vOc975FU9RfXqfNE\n4Td/37g930P2PTbCvZc8lgVvH9YQUu0PTRHNKpX5ZNfoFoWhFxQUFAwI+s7QX3jhhRaj8RVvGJt/\nMkOmK8EcCwv2lWUYODMc/qukanXbODOmJ+yZzDvEy2Ex++Kt4j6mzNSe1OmUU06RVDEgZm1nMc5W\npE5G4yWuPIVvF37ou4T0nCQcIhaAvncvgjpbPnLLRd950ir36c4VA/dSdWnSJm+/e6e4vzIapkc4\nIwv36nGGyydjjXuu85mmnYwJxit+zHXFkHuJGGPH2odH/qL1ct+k200ZumslngyPvkU+3I+XakPr\nQ65EpXI8Gnkds3Vt3J+DXFwM48GjxpG7ywqGnosklSqNMOcR5d4/3aIw9IKCgoIBQd9L0MUYO8q9\nMZN7AnlmSqLl0ihN2CoMHbbjLJdzwIKYwcnxwfH8jk2S/XNFN6TOGd1LWKE1eDpWmBzMlf29KIf7\n+/KZetrAAr3kGtfA+8N9hmcC7o9PylTsnF4sxD0TpPaSainoE+TgfeE5PLzPsHd7DhS/jtRZOBqZ\nIye3qQKPOvYiKc7o3a87zWnjBalZW6DgB99Tu3Cv10bwSnOG6j7iaA233367pGospvdDfztD96IT\nXAMtCNs4rJdUxHzSBvLqeHm8dLxzTeSbs0/zuz/7HgvhZSqRmRdDT+NnWAOAoTsT93wx00Vh6AUF\nBQUDgtDPXNYhhKclbZX0zFT7zjIO1GC38YgY48t71ZAi156iyHX6GHS5Sl3Ktq8vdElqFkRY1deL\nThOljdPHXGtPHUobp4+51p46lDZWKCaXgoKCggFBeaEXFBQUDAhm44V+9Sxcc7oobZw+5lp76lDa\nOH3MtfbUobSxib7b0AsKCgoKZgbF5FJQUFAwICgv9IKCgoIBQd9e6CGEN4UQ7g8hbAghfKJf150M\nIYTDQgi3hBDWhRDuDSH8YfP3ZSGE74UQHmx+Lp0DbR0OIdwZQviX5vcjQwi3NfvzKyGEzoQR/Wvb\nnJJtkWvP2jan5CrtObKdLbn25YUeQhiW9LeSLpK0UtIVIYSV/bj2FBiX9EcxxpWSzpL0kWa7PiHp\nphjjsZJuan6fbfyhpPXJ97+Q9FcxxmMkbZL0gdlo1ByVbZHrbmKOylXac2Q7O3KlzudM/kk6W9KN\nyfdPSvpkP649zXZ+U9LvSLpf0vLmb8sl3T/L7TpUjUH6Bkn/IimoEXU2r65/+9y2OS/bItfBlOtc\nle1syrVfJpdDJKX11B5r/jZnEEJYIek0SbdJOjjG+GRz01OSDp6lZoG/lvTHksjYc4Ck52OMZIea\nzf6c07Itct1lzGm5SnNatrMm17IoKimEsETS1yR9LMbYVqIoNqbUWfPtDCG8RdLGGOPPZ6sNeyqK\nXAcXc1W2sy3XfqXPfVzSYcn3Q5u/zTpCCPPVGBhfjjF+vfnzb0IIy2OMT4YQlkvamD/DjON1ki4N\nIbxZ0kJJ+0r6tKT9QwjzmrP+bPbnnJRtketuY07KVZrzsp1VufaLof9M0rHNld4Fkt4p6fo+XTuL\n0EhG/DlJ62OMf5lsul7Slc3/r1TDTjcriDF+MsZ4aIxxhRr9dnOM8d2SbpH0juZus9nGOSfbItee\nYM7JVZr7sp11ufZxoeDNkh6Q9JCk/zhbCxbWpteroZrdJWlt8+/Nati8bpL0oKTvS1o2221ttvd8\nSf/S/P8oSbdL2iDpq5JGZrFdc0q2Ra6DKdc9TbazIdcS+l9QUFAwICiLogUFBQUDgvJCLygoKBgQ\nlBd6QUFBwYCgvNALCgoKBgTlhV5QUFAwICgv9IKCgoIBQXmhFxQUFAwI/j+0ajtpH+Ea7QAAAABJ\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1df91b70ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAACFCAYAAABL2gNbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nNEWYXD0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1df84b2d7f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(train_images.shape, train_labels.shape)\n",
    "\n",
    "# Show samples from training data\n",
    "images = orig_images.reshape(train_images.shape[0], 48, 48)\n",
    "zca_images = train_images.reshape(train_images.shape[0], 48, 48)\n",
    "n = 3\n",
    "row = 3\n",
    "labels = ['Angry', 'Happy', 'Neutral']\n",
    "begin = 0 # can be random, but pick a number that reproduces results\n",
    "for i in range(begin, begin + 3):\n",
    "    plt.subplot(n//row, row, i%row+1)\n",
    "    plt.imshow(images[i], cmap=plt.get_cmap('gray'))\n",
    "    plt.title(labels[int(train_labels[i])])\n",
    "plt.show()\n",
    "for i in range(begin, begin + 3):\n",
    "    plt.subplot(n//row, row, i%row+1)\n",
    "    plt.imshow(zca_images[i], cmap=plt.get_cmap('gray'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split(X, Y, train_index, test_index):\n",
    "    X, X_val = X[train_index], X[test_index]\n",
    "    Y, Y_val = Y[train_index], Y[test_index]\n",
    "\n",
    "    # Augment data\n",
    "    \n",
    "    train_aug_dir='../Hw2_data/images/training_augmented'\n",
    "    if not os.path.exists(train_aug_dir):\n",
    "        os.makedirs(train_aug_dir)\n",
    "    test_aug_dir='../Hw2_data/images/test_augmented'\n",
    "    if not os.path.exists(test_aug_dir):\n",
    "        os.makedirs(test_aug_dir)\n",
    "\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        data_format='channels_first'\n",
    "    )\n",
    "    train_datagen.fit(X)\n",
    "    train_generator = train_datagen.flow(\n",
    "        X,\n",
    "        Y,\n",
    "        seed=42,\n",
    "        batch_size=batch_size,\n",
    "#         save_to_dir=train_aug_dir # uncomment to see Data\n",
    "    )\n",
    "\n",
    "    validate_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        data_format='channels_first'\n",
    "    )\n",
    "    validate_datagen.fit(X_val)\n",
    "    validate_generator = train_datagen.flow(\n",
    "        X_val,\n",
    "        Y_val,\n",
    "        seed=42,\n",
    "        batch_size=batch_size,\n",
    "#         save_to_dir=test_aug_dir # uncomment to see Data\n",
    "    )\n",
    "    \n",
    "    return (X, Y, X_val, Y_val, train_generator, validate_generator)\n",
    "\n",
    "def train(model, aug_mult=2, epochs=20, batch_size=128):\n",
    "    \"\"\"\n",
    "    Train a model\n",
    "    \"\"\"\n",
    "    train_size = round(X_train.shape[0] * aug_mult)\n",
    "    validate_size = round(X_validate.shape[0] * aug_mult)\n",
    "    print('Augmented training split: ', train_size)\n",
    "    print('Augmented test split: ', validate_size)\n",
    "    \n",
    "    # Adjust Learning Rate\n",
    "    reduceLR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, verbose=1)\n",
    "    \n",
    "    # Early stopping\n",
    "    earlyStopper = EarlyStopping(verbose=1, patience=15)\n",
    "    \n",
    "    # Begin train\n",
    "    history = model.fit_generator(\n",
    "        train_generator, \n",
    "        epochs=epochs,\n",
    "        steps_per_epoch=train_size // batch_size,\n",
    "        validation_data=validate_generator,\n",
    "        validation_steps=validate_size // batch_size,\n",
    "        verbose=1,\n",
    "        callbacks=[tbCallback, checkpoint, reduceLR, earlyStopper],\n",
    "        workers=4\n",
    "    )\n",
    "    \n",
    "    # Save the model\n",
    "    model.save('../Hw2_data/model.h5')\n",
    "    \n",
    "    val_acc = max(history.history['val_acc'])\n",
    "    print('Max val_acc: %.2f%%' % (val_acc * 100))\n",
    "    return val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createFineTuneModel(lr=1e-4, batch_size=128):\n",
    "    \"\"\"\n",
    "    Create a Model for fine tuning the top classifier\n",
    "    \"\"\"\n",
    "    input_tensor = Input(shape=(48, 48, 3))\n",
    "    base_model = VGG19(weights='imagenet', include_top=False, input_tensor=input_tensor)\n",
    "#     top_model.add(Flatten(input_shape=base_model.output_shape[1:]))\n",
    "    X = base_model.output\n",
    "    X = pooling.GlobalAveragePooling2D()(X)\n",
    "#     X = Flatten(input_shape=base_model.output_shape[1:])(X)\n",
    "    X = Dense(256, activation='relu')(X)\n",
    "    predictions = Dense(3, activation='softmax')(X)\n",
    "        \n",
    "    model = Model(input=base_model.input, output=predictions)\n",
    "    for layer in base_model.layers:#[:15]:\n",
    "        layer.trainable = False\n",
    "    model.compile(\n",
    "        optimizer=SGD(lr, momentum=0.9),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def createModel(stacks=5, base_filter_size=32, lr=0.003):\n",
    "    \"\"\"\n",
    "    Deep CNN\n",
    "    \n",
    "    Creates a Small Neural Net made for training small images batches\n",
    "    \n",
    "    References: \n",
    "    https://shuaiw.github.io/2017/01/24/deep-learning-for-logo-recognition-part-i.html\n",
    "    http://lamda.nju.edu.cn/weixs/project/CNNTricks/CNNTricks.html\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "\n",
    "    # one limitation is that you can't specify which stacks need dropout, batch normalization, \n",
    "    # it is something I'll work on in the future\n",
    "    for i in range(0, stacks):\n",
    "        model.add(convolutional.Conv2D(\n",
    "            filters=base_filter_size, \n",
    "            strides=(1,1),\n",
    "            kernel_size=(3,3),\n",
    "            input_shape=(1,48,48),\n",
    "            data_format='channels_first',\n",
    "            padding='same',\n",
    "            kernel_initializer=VarianceScaling(scale=2.0, seed=42),\n",
    "#             kernel_regularizer=l2(0.001)\n",
    "        ))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(convolutional.Conv2D(\n",
    "            filters=base_filter_size,\n",
    "            strides=(1,1),\n",
    "            kernel_size=(3,3),\n",
    "            padding='same',\n",
    "            kernel_initializer=VarianceScaling(scale=2.0, seed=42),\n",
    "#             kernel_regularizer=l2(0.001)\n",
    "        ))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(pooling.MaxPooling2D(\n",
    "            pool_size=(2,2),\n",
    "            padding='same'\n",
    "        ))\n",
    "        model.add(normalization.BatchNormalization())\n",
    "        if base_filter_size < 256: # my GPU can't handle filters bigger than 256\n",
    "            base_filter_size *= 2\n",
    "\n",
    "    model.add(convolutional.Conv2D(\n",
    "        filters=256,\n",
    "        strides=(1,1),\n",
    "        kernel_size=(3,3),\n",
    "        padding='same',\n",
    "        kernel_initializer=VarianceScaling(scale=2.0, seed=42),\n",
    "#             kernel_regularizer=l2(0.001)\n",
    "    ))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(pooling.MaxPooling2D(\n",
    "        pool_size=(2,2),\n",
    "        padding='same'\n",
    "    ))\n",
    "    model.add(normalization.BatchNormalization())\n",
    "            \n",
    "    # Flatten\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(128, kernel_initializer=VarianceScaling(scale=2.0, seed=42)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(64, kernel_initializer=VarianceScaling(scale=2.0, seed=42)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    # FC layer with softmax\n",
    "    model.add(Dense(3))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    # Use RMSprop + momentum\n",
    "    opt = Adam(lr)\n",
    "\n",
    "    # Choose loss function, optimization method, and metrics to display\n",
    "    model.compile(\n",
    "        optimizer = opt,\n",
    "        loss = 'categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_size = 0.1 # since we augment the data to create a larger set, this should never really be changed\n",
    "batch_size = 128 # This value seems good for GTX 980 4GB DDR3 without maxing out mem\n",
    "aug_mult = 1.25 # increasing this will allow larger data, but increase epoch time\n",
    "\n",
    "# Reshape\n",
    "X = train_images.reshape(-1,1,48,48)    \n",
    "Y = np_utils.to_categorical(train_labels, 3)\n",
    "\n",
    "# Save best model\n",
    "checkpoint = ModelCheckpoint('../Hw2_data/weights.best.hdf5', monitor='val_acc', verbose=1, save_best_only=True, mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "runId = 31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train #31: \n",
      "Train index:  [11961 10583 12555 ...,  8541 11607  3760]\n",
      "Test index:  [ 3391  2079 16077 ...,  2741  4778 10531]\n",
      "Augmented training split:  18196\n",
      "Augmented test split:  2022\n",
      "Epoch 1/60\n",
      "141/142 [============================>.] - ETA: 0s - loss: 1.1861 - acc: 0.4149Epoch 00000: val_acc improved from -inf to 0.44984, saving model to ../Hw2_data/weights.best.hdf5\n",
      "142/142 [==============================] - 72s - loss: 1.1852 - acc: 0.4152 - val_loss: 1.0803 - val_acc: 0.4498\n",
      "Epoch 2/60\n",
      "141/142 [============================>.] - ETA: 0s - loss: 1.0783 - acc: 0.4471Epoch 00001: val_acc improved from 0.44984 to 0.45945, saving model to ../Hw2_data/weights.best.hdf5\n",
      "142/142 [==============================] - 105s - loss: 1.0782 - acc: 0.4473 - val_loss: 1.0615 - val_acc: 0.4594\n",
      "Epoch 3/60\n",
      "141/142 [============================>.] - ETA: 0s - loss: 1.0704 - acc: 0.4412Epoch 00002: val_acc improved from 0.45945 to 0.46211, saving model to ../Hw2_data/weights.best.hdf5\n",
      "142/142 [==============================] - 118s - loss: 1.0702 - acc: 0.4416 - val_loss: 1.2014 - val_acc: 0.4621\n",
      "Epoch 4/60\n",
      "141/142 [============================>.] - ETA: 0s - loss: 1.0637 - acc: 0.4421Epoch 00003: val_acc did not improve\n",
      "142/142 [==============================] - 125s - loss: 1.0638 - acc: 0.4423 - val_loss: 1.1239 - val_acc: 0.4440\n",
      "Epoch 5/60\n",
      "141/142 [============================>.] - ETA: 0s - loss: 1.0643 - acc: 0.4447Epoch 00004: val_acc did not improve\n",
      "142/142 [==============================] - 127s - loss: 1.0642 - acc: 0.4449 - val_loss: 1.0277 - val_acc: 0.4616\n",
      "Epoch 6/60\n",
      "141/142 [============================>.] - ETA: 0s - loss: 1.0611 - acc: 0.4409Epoch 00005: val_acc did not improve\n",
      "142/142 [==============================] - 127s - loss: 1.0610 - acc: 0.4412 - val_loss: 1.0399 - val_acc: 0.4472\n",
      "Epoch 7/60\n",
      "141/142 [============================>.] - ETA: 0s - loss: 1.0531 - acc: 0.4429Epoch 00006: val_acc did not improve\n",
      "142/142 [==============================] - 127s - loss: 1.0530 - acc: 0.4432 - val_loss: 1.0211 - val_acc: 0.4424\n",
      "Epoch 8/60\n",
      "141/142 [============================>.] - ETA: 0s - loss: 1.0496 - acc: 0.4429Epoch 00007: val_acc did not improve\n",
      "142/142 [==============================] - 128s - loss: 1.0498 - acc: 0.4426 - val_loss: 1.0257 - val_acc: 0.4488\n",
      "Epoch 9/60\n",
      "141/142 [============================>.] - ETA: 0s - loss: 1.0424 - acc: 0.4453Epoch 00008: val_acc did not improve\n",
      "142/142 [==============================] - 126s - loss: 1.0429 - acc: 0.4449 - val_loss: 1.0013 - val_acc: 0.4616\n",
      "Epoch 10/60\n",
      "141/142 [============================>.] - ETA: 0s - loss: 1.0426 - acc: 0.4423Epoch 00009: val_acc did not improve\n",
      "142/142 [==============================] - 127s - loss: 1.0422 - acc: 0.4424 - val_loss: 1.0189 - val_acc: 0.4606\n",
      "Epoch 11/60\n",
      "141/142 [============================>.] - ETA: 0s - loss: 1.0373 - acc: 0.4436Epoch 00010: val_acc did not improve\n",
      "142/142 [==============================] - 125s - loss: 1.0375 - acc: 0.4430 - val_loss: 1.0152 - val_acc: 0.4535\n",
      "Epoch 12/60\n",
      "141/142 [============================>.] - ETA: 0s - loss: 1.0364 - acc: 0.4421Epoch 00011: val_acc improved from 0.46211 to 0.46371, saving model to ../Hw2_data/weights.best.hdf5\n",
      "142/142 [==============================] - 125s - loss: 1.0366 - acc: 0.4424 - val_loss: 1.0463 - val_acc: 0.4637\n",
      "Epoch 13/60\n",
      "141/142 [============================>.] - ETA: 0s - loss: 1.0276 - acc: 0.4425Epoch 00012: val_acc did not improve\n",
      "142/142 [==============================] - 125s - loss: 1.0275 - acc: 0.4424 - val_loss: 0.9840 - val_acc: 0.4461\n",
      "Epoch 14/60\n",
      "141/142 [============================>.] - ETA: 0s - loss: 1.0239 - acc: 0.4440Epoch 00013: val_acc did not improve\n",
      "142/142 [==============================] - 125s - loss: 1.0240 - acc: 0.4434 - val_loss: 1.0283 - val_acc: 0.4600\n",
      "Epoch 15/60\n",
      "141/142 [============================>.] - ETA: 0s - loss: 1.0161 - acc: 0.4455Epoch 00014: val_acc did not improve\n",
      "142/142 [==============================] - 125s - loss: 1.0163 - acc: 0.4453 - val_loss: 0.9903 - val_acc: 0.4584\n",
      "Epoch 16/60\n",
      "141/142 [============================>.] - ETA: 0s - loss: 1.0138 - acc: 0.4442Epoch 00015: val_acc did not improve\n",
      "142/142 [==============================] - 125s - loss: 1.0135 - acc: 0.4438 - val_loss: 0.9828 - val_acc: 0.4573\n",
      "Epoch 17/60\n",
      "141/142 [============================>.] - ETA: 0s - loss: 1.0042 - acc: 0.4457Epoch 00016: val_acc did not improve\n",
      "142/142 [==============================] - 124s - loss: 1.0042 - acc: 0.4459 - val_loss: 0.9813 - val_acc: 0.4552\n",
      "Epoch 18/60\n",
      "141/142 [============================>.] - ETA: 0s - loss: 1.0062 - acc: 0.4466Epoch 00017: val_acc did not improve\n",
      "142/142 [==============================] - 125s - loss: 1.0066 - acc: 0.4465 - val_loss: 0.9462 - val_acc: 0.4616\n",
      "Epoch 19/60\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.9977 - acc: 0.4547Epoch 00018: val_acc improved from 0.46371 to 0.55088, saving model to ../Hw2_data/weights.best.hdf5\n",
      "142/142 [==============================] - 125s - loss: 0.9976 - acc: 0.4551 - val_loss: 0.9532 - val_acc: 0.5509\n",
      "Epoch 20/60\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.9950 - acc: 0.4679Epoch 00019: val_acc did not improve\n",
      "142/142 [==============================] - 125s - loss: 0.9952 - acc: 0.4678 - val_loss: 0.9794 - val_acc: 0.5384\n",
      "Epoch 21/60\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.9964 - acc: 0.4806Epoch 00020: val_acc improved from 0.55088 to 0.55390, saving model to ../Hw2_data/weights.best.hdf5\n",
      "142/142 [==============================] - 125s - loss: 0.9965 - acc: 0.4805 - val_loss: 0.9682 - val_acc: 0.5539\n",
      "Epoch 22/60\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.9833 - acc: 0.4935Epoch 00021: val_acc did not improve\n",
      "142/142 [==============================] - 125s - loss: 0.9836 - acc: 0.4932 - val_loss: 0.9739 - val_acc: 0.5502\n",
      "Epoch 23/60\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.9764 - acc: 0.5065Epoch 00022: val_acc improved from 0.55390 to 0.56937, saving model to ../Hw2_data/weights.best.hdf5\n",
      "142/142 [==============================] - 125s - loss: 0.9763 - acc: 0.5071 - val_loss: 0.9457 - val_acc: 0.5694\n",
      "Epoch 24/60\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.9763 - acc: 0.5063Epoch 00023: val_acc did not improve\n",
      "142/142 [==============================] - 125s - loss: 0.9760 - acc: 0.5062 - val_loss: 0.9303 - val_acc: 0.5592\n",
      "Epoch 25/60\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.9752 - acc: 0.5023Epoch 00024: val_acc did not improve\n",
      "142/142 [==============================] - 125s - loss: 0.9751 - acc: 0.5025 - val_loss: 0.9526 - val_acc: 0.5688\n",
      "Epoch 26/60\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.9640 - acc: 0.5234Epoch 00025: val_acc improved from 0.56937 to 0.56990, saving model to ../Hw2_data/weights.best.hdf5\n",
      "142/142 [==============================] - 128s - loss: 0.9641 - acc: 0.5233 - val_loss: 0.9355 - val_acc: 0.5699\n",
      "Epoch 27/60\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.9559 - acc: 0.5288Epoch 00026: val_acc did not improve\n",
      "142/142 [==============================] - 127s - loss: 0.9561 - acc: 0.5290 - val_loss: 0.9363 - val_acc: 0.5630\n",
      "Epoch 28/60\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.9530 - acc: 0.5245Epoch 00027: val_acc did not improve\n",
      "142/142 [==============================] - 127s - loss: 0.9535 - acc: 0.5247 - val_loss: 0.9334 - val_acc: 0.5629\n",
      "Epoch 29/60\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.9448 - acc: 0.5324Epoch 00028: val_acc improved from 0.56990 to 0.57166, saving model to ../Hw2_data/weights.best.hdf5\n",
      "142/142 [==============================] - 128s - loss: 0.9448 - acc: 0.5327 - val_loss: 0.9171 - val_acc: 0.5717\n",
      "Epoch 30/60\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.9483 - acc: 0.5285Epoch 00029: val_acc did not improve\n",
      "142/142 [==============================] - 128s - loss: 0.9487 - acc: 0.5284 - val_loss: 0.9412 - val_acc: 0.5587\n",
      "Epoch 31/60\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.9351 - acc: 0.5431Epoch 00030: val_acc did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142/142 [==============================] - 128s - loss: 0.9348 - acc: 0.5439 - val_loss: 0.9155 - val_acc: 0.5672\n",
      "Epoch 32/60\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.9314 - acc: 0.5457Epoch 00031: val_acc improved from 0.57166 to 0.58378, saving model to ../Hw2_data/weights.best.hdf5\n",
      "142/142 [==============================] - 128s - loss: 0.9315 - acc: 0.5457 - val_loss: 0.9149 - val_acc: 0.5838\n",
      "Epoch 33/60\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.9230 - acc: 0.5500Epoch 00032: val_acc improved from 0.58378 to 0.59552, saving model to ../Hw2_data/weights.best.hdf5\n",
      "142/142 [==============================] - 128s - loss: 0.9231 - acc: 0.5497 - val_loss: 0.9174 - val_acc: 0.5955\n",
      "Epoch 34/60\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.9189 - acc: 0.5508Epoch 00033: val_acc improved from 0.59552 to 0.60139, saving model to ../Hw2_data/weights.best.hdf5\n",
      "142/142 [==============================] - 128s - loss: 0.9186 - acc: 0.5512 - val_loss: 0.8985 - val_acc: 0.6014\n",
      "Epoch 35/60\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.9106 - acc: 0.5556Epoch 00034: val_acc did not improve\n",
      "142/142 [==============================] - 128s - loss: 0.9102 - acc: 0.5556 - val_loss: 0.8999 - val_acc: 0.5843\n",
      "Epoch 36/60\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.9102 - acc: 0.5567Epoch 00035: val_acc did not improve\n",
      "142/142 [==============================] - 128s - loss: 0.9105 - acc: 0.5565 - val_loss: 0.9044 - val_acc: 0.5919\n",
      "Epoch 37/60\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.9029 - acc: 0.5593Epoch 00036: val_acc did not improve\n",
      "142/142 [==============================] - 128s - loss: 0.9029 - acc: 0.5595 - val_loss: 0.9139 - val_acc: 0.5726\n",
      "Epoch 38/60\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.9042 - acc: 0.5557Epoch 00037: val_acc improved from 0.60139 to 0.61900, saving model to ../Hw2_data/weights.best.hdf5\n",
      "142/142 [==============================] - 128s - loss: 0.9036 - acc: 0.5565 - val_loss: 0.8538 - val_acc: 0.6190\n",
      "Epoch 39/60\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.8959 - acc: 0.5693Epoch 00038: val_acc did not improve\n",
      "142/142 [==============================] - 128s - loss: 0.8961 - acc: 0.5692 - val_loss: 0.8723 - val_acc: 0.6057\n",
      "Epoch 40/60\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.8840 - acc: 0.5641Epoch 00039: val_acc did not improve\n",
      "142/142 [==============================] - 128s - loss: 0.8845 - acc: 0.5639 - val_loss: 0.8905 - val_acc: 0.5832\n",
      "Epoch 41/60\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.8873 - acc: 0.5637Epoch 00040: val_acc did not improve\n",
      "142/142 [==============================] - 128s - loss: 0.8875 - acc: 0.5637 - val_loss: 0.8644 - val_acc: 0.6089\n",
      "Epoch 42/60\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.8880 - acc: 0.5702Epoch 00041: val_acc did not improve\n",
      "142/142 [==============================] - 128s - loss: 0.8883 - acc: 0.5700 - val_loss: 0.8806 - val_acc: 0.6030\n",
      "Epoch 43/60\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.8752 - acc: 0.5729Epoch 00042: val_acc did not improve\n",
      "142/142 [==============================] - 128s - loss: 0.8756 - acc: 0.5729 - val_loss: 0.8747 - val_acc: 0.5966\n",
      "Epoch 44/60\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.8700 - acc: 0.5810Epoch 00043: val_acc improved from 0.61900 to 0.62199, saving model to ../Hw2_data/weights.best.hdf5\n",
      "142/142 [==============================] - 129s - loss: 0.8705 - acc: 0.5808 - val_loss: 0.8527 - val_acc: 0.6220\n",
      "Epoch 45/60\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.8576 - acc: 0.5853Epoch 00044: val_acc did not improve\n",
      "142/142 [==============================] - 128s - loss: 0.8578 - acc: 0.5851 - val_loss: 0.8673 - val_acc: 0.6094\n",
      "Epoch 46/60\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.8558 - acc: 0.5887Epoch 00045: val_acc did not improve\n",
      "142/142 [==============================] - 128s - loss: 0.8561 - acc: 0.5886 - val_loss: 0.8751 - val_acc: 0.6061\n",
      "Epoch 47/60\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.8441 - acc: 0.5958Epoch 00046: val_acc did not improve\n",
      "142/142 [==============================] - 128s - loss: 0.8441 - acc: 0.5959 - val_loss: 0.8420 - val_acc: 0.6174\n",
      "Epoch 48/60\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.8399 - acc: 0.6021Epoch 00047: val_acc did not improve\n",
      "142/142 [==============================] - 128s - loss: 0.8398 - acc: 0.6019 - val_loss: 0.9034 - val_acc: 0.5827\n",
      "Epoch 49/60\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.8367 - acc: 0.6039Epoch 00048: val_acc did not improve\n",
      "142/142 [==============================] - 128s - loss: 0.8363 - acc: 0.6042 - val_loss: 0.8434 - val_acc: 0.6030\n",
      "Epoch 50/60\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.8238 - acc: 0.6089Epoch 00049: val_acc did not improve\n",
      "142/142 [==============================] - 128s - loss: 0.8237 - acc: 0.6086 - val_loss: 0.8378 - val_acc: 0.6083\n",
      "Epoch 51/60\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.8212 - acc: 0.6102Epoch 00050: val_acc did not improve\n",
      "142/142 [==============================] - 128s - loss: 0.8215 - acc: 0.6100 - val_loss: 0.8269 - val_acc: 0.6078\n",
      "Epoch 52/60\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.8169 - acc: 0.6093Epoch 00051: val_acc did not improve\n",
      "142/142 [==============================] - 128s - loss: 0.8172 - acc: 0.6088 - val_loss: 0.8305 - val_acc: 0.6030\n",
      "Epoch 53/60\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.8034 - acc: 0.6159Epoch 00052: val_acc did not improve\n",
      "142/142 [==============================] - 128s - loss: 0.8035 - acc: 0.6158 - val_loss: 0.8468 - val_acc: 0.6046\n",
      "Epoch 54/60\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.8049 - acc: 0.6176Epoch 00053: val_acc did not improve\n",
      "142/142 [==============================] - 128s - loss: 0.8049 - acc: 0.6175 - val_loss: 0.8604 - val_acc: 0.6045\n",
      "Epoch 55/60\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.8018 - acc: 0.6138Epoch 00054: val_acc did not improve\n",
      "142/142 [==============================] - 128s - loss: 0.8017 - acc: 0.6137 - val_loss: 0.8149 - val_acc: 0.6023\n",
      "Epoch 56/60\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.7934 - acc: 0.6183Epoch 00055: val_acc did not improve\n",
      "142/142 [==============================] - 128s - loss: 0.7936 - acc: 0.6182 - val_loss: 0.8318 - val_acc: 0.6121\n",
      "Epoch 57/60\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.7824 - acc: 0.6252Epoch 00056: val_acc did not improve\n",
      "142/142 [==============================] - 128s - loss: 0.7830 - acc: 0.6251 - val_loss: 0.8362 - val_acc: 0.6206\n",
      "Epoch 58/60\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.7706 - acc: 0.6308Epoch 00057: val_acc did not improve\n",
      "142/142 [==============================] - 128s - loss: 0.7711 - acc: 0.6304 - val_loss: 0.7872 - val_acc: 0.6169\n",
      "Epoch 59/60\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.7766 - acc: 0.6270Epoch 00058: val_acc did not improve\n",
      "142/142 [==============================] - 129s - loss: 0.7770 - acc: 0.6271 - val_loss: 0.8253 - val_acc: 0.6115\n",
      "Epoch 60/60\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.7674 - acc: 0.6301Epoch 00059: val_acc improved from 0.62199 to 0.62647, saving model to ../Hw2_data/weights.best.hdf5\n",
      "142/142 [==============================] - 128s - loss: 0.7676 - acc: 0.6299 - val_loss: 0.8046 - val_acc: 0.6265\n",
      "Max val_acc: 62.65%\n",
      "Average val_acc: 62.65%\n"
     ]
    }
   ],
   "source": [
    "# train k-folds and average the validation accuracy\n",
    "k = 1\n",
    "total_val_acc = 0\n",
    "sss = StratifiedShuffleSplit(n_splits=k, test_size=test_size, random_state=42)\n",
    "\n",
    "# Split in a stratified way to make sure the training data covers all classes\n",
    "for train_index, test_index in sss.split(X, Y):\n",
    "    print('Train #%d: ' % runId)\n",
    "    print('Train index: ', train_index)\n",
    "    print('Test index: ', test_index)\n",
    "    \n",
    "    X_train, Y_train, X_validate, Y_validate, train_generator, validate_generator = split(X, Y, train_index, test_index)\n",
    "    \n",
    "    # Show loss and accuracy at each epoch in Tensorboard\n",
    "    dir = './Graph/%d' % runId\n",
    "    tbCallback = TensorBoard(log_dir=dir, histogram_freq=0, write_graph=True, write_images=True)\n",
    "    \n",
    "#     model = createFineTuneModel(\n",
    "#         lr=1e-4\n",
    "#     )\n",
    "#     total_val_acc += train(\n",
    "#         model,\n",
    "#         aug_mult=aug_mult, \n",
    "#         epochs=5, # train on a few epochs\n",
    "#         batch_size=batch_size\n",
    "#     )\n",
    "    \n",
    "    model = createModel(\n",
    "        stacks=4, # 2 Conv per stack\n",
    "        base_filter_size=32, # increases by 2 per stack\n",
    "        lr=0.0001#0.00001875 # based on batch normalization and dropout. Find a good learning rate starting from 0.0003\n",
    "    )\n",
    "    total_val_acc += train(\n",
    "        model, \n",
    "        aug_mult=aug_mult, \n",
    "        epochs=60, # any number since we can early stop based on validation loss\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "        \n",
    "    runId += 1\n",
    "\n",
    "print ('Average val_acc: %.2f%%' % (total_val_acc / k * 100))\n",
    "    \n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented training split:  18196\n",
      "Augmented test split:  2022\n",
      "Epoch 1/35\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.7597 - acc: 0.6321Epoch 00000: val_acc improved from 0.62647 to 0.63607, saving model to ../Hw2_data/weights.best.hdf5\n",
      "142/142 [==============================] - 82s - loss: 0.7594 - acc: 0.6322 - val_loss: 0.7914 - val_acc: 0.6361\n",
      "Epoch 2/35\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.7522 - acc: 0.6370Epoch 00001: val_acc did not improve\n",
      "142/142 [==============================] - 113s - loss: 0.7519 - acc: 0.6373 - val_loss: 0.8937 - val_acc: 0.5736\n",
      "Epoch 3/35\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.7553 - acc: 0.6357Epoch 00002: val_acc did not improve\n",
      "142/142 [==============================] - 125s - loss: 0.7556 - acc: 0.6356 - val_loss: 0.8185 - val_acc: 0.6062\n",
      "Epoch 4/35\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.7543 - acc: 0.6380Epoch 00003: val_acc did not improve\n",
      "142/142 [==============================] - 127s - loss: 0.7550 - acc: 0.6376 - val_loss: 0.8392 - val_acc: 0.6127\n",
      "Epoch 5/35\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.7392 - acc: 0.6431Epoch 00004: val_acc did not improve\n",
      "142/142 [==============================] - 127s - loss: 0.7394 - acc: 0.6430 - val_loss: 0.7971 - val_acc: 0.6329\n",
      "Epoch 6/35\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.7325 - acc: 0.6448Epoch 00005: val_acc did not improve\n",
      "142/142 [==============================] - 128s - loss: 0.7324 - acc: 0.6448 - val_loss: 0.7802 - val_acc: 0.6291\n",
      "Epoch 7/35\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.7358 - acc: 0.6434Epoch 00006: val_acc did not improve\n",
      "142/142 [==============================] - 128s - loss: 0.7356 - acc: 0.6435 - val_loss: 0.7807 - val_acc: 0.6313\n",
      "Epoch 8/35\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.7295 - acc: 0.6452Epoch 00007: val_acc did not improve\n",
      "142/142 [==============================] - 128s - loss: 0.7295 - acc: 0.6449 - val_loss: 0.7791 - val_acc: 0.6222\n",
      "Epoch 9/35\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.7059 - acc: 0.6561Epoch 00008: val_acc did not improve\n",
      "142/142 [==============================] - 128s - loss: 0.7062 - acc: 0.6556 - val_loss: 0.7923 - val_acc: 0.6265\n",
      "Epoch 10/35\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.7075 - acc: 0.6585Epoch 00009: val_acc did not improve\n",
      "142/142 [==============================] - 128s - loss: 0.7078 - acc: 0.6585 - val_loss: 0.7619 - val_acc: 0.6318\n",
      "Epoch 11/35\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.7104 - acc: 0.6555Epoch 00010: val_acc did not improve\n",
      "142/142 [==============================] - 128s - loss: 0.7103 - acc: 0.6552 - val_loss: 0.7709 - val_acc: 0.6243\n",
      "Epoch 12/35\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.7021 - acc: 0.6568Epoch 00011: val_acc did not improve\n",
      "142/142 [==============================] - 128s - loss: 0.7023 - acc: 0.6564 - val_loss: 0.7624 - val_acc: 0.6307\n",
      "Epoch 13/35\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.7058 - acc: 0.6555Epoch 00012: val_acc did not improve\n",
      "142/142 [==============================] - 128s - loss: 0.7051 - acc: 0.6557 - val_loss: 0.7446 - val_acc: 0.6334\n",
      "Epoch 14/35\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.6949 - acc: 0.6598Epoch 00013: val_acc did not improve\n",
      "142/142 [==============================] - 128s - loss: 0.6955 - acc: 0.6597 - val_loss: 0.7737 - val_acc: 0.6350\n",
      "Epoch 15/35\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.6947 - acc: 0.6594Epoch 00014: val_acc did not improve\n",
      "142/142 [==============================] - 128s - loss: 0.6944 - acc: 0.6595 - val_loss: 0.7648 - val_acc: 0.6329\n",
      "Epoch 16/35\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.6880 - acc: 0.6613Epoch 00015: val_acc did not improve\n",
      "142/142 [==============================] - 127s - loss: 0.6881 - acc: 0.6613 - val_loss: 0.8150 - val_acc: 0.6142\n",
      "Epoch 17/35\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.6798 - acc: 0.6665Epoch 00016: val_acc did not improve\n",
      "142/142 [==============================] - 127s - loss: 0.6807 - acc: 0.6663 - val_loss: 0.7511 - val_acc: 0.6355\n",
      "Epoch 18/35\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.6801 - acc: 0.6654Epoch 00017: val_acc improved from 0.63607 to 0.64194, saving model to ../Hw2_data/weights.best.hdf5\n",
      "142/142 [==============================] - 128s - loss: 0.6802 - acc: 0.6651 - val_loss: 0.7652 - val_acc: 0.6419\n",
      "Epoch 19/35\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.6850 - acc: 0.6606Epoch 00018: val_acc did not improve\n",
      "142/142 [==============================] - 128s - loss: 0.6845 - acc: 0.6607 - val_loss: 0.8303 - val_acc: 0.6115\n",
      "Epoch 20/35\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.6680 - acc: 0.6722Epoch 00019: val_acc improved from 0.64194 to 0.64621, saving model to ../Hw2_data/weights.best.hdf5\n",
      "142/142 [==============================] - 128s - loss: 0.6679 - acc: 0.6721 - val_loss: 0.7504 - val_acc: 0.6462\n",
      "Epoch 21/35\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.6622 - acc: 0.6722Epoch 00020: val_acc did not improve\n",
      "142/142 [==============================] - 128s - loss: 0.6625 - acc: 0.6721 - val_loss: 0.7712 - val_acc: 0.6318\n",
      "Epoch 22/35\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.6616 - acc: 0.6732Epoch 00021: val_acc did not improve\n",
      "142/142 [==============================] - 128s - loss: 0.6618 - acc: 0.6733 - val_loss: 0.7636 - val_acc: 0.6275\n",
      "Epoch 23/35\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.6571 - acc: 0.6769Epoch 00022: val_acc did not improve\n",
      "142/142 [==============================] - 128s - loss: 0.6576 - acc: 0.6767 - val_loss: 0.7579 - val_acc: 0.6393\n",
      "Epoch 24/35\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.6545 - acc: 0.6744Epoch 00023: val_acc improved from 0.64621 to 0.64674, saving model to ../Hw2_data/weights.best.hdf5\n",
      "142/142 [==============================] - 129s - loss: 0.6548 - acc: 0.6743 - val_loss: 0.7321 - val_acc: 0.6467\n",
      "Epoch 25/35\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.6472 - acc: 0.6801Epoch 00024: val_acc improved from 0.64674 to 0.65795, saving model to ../Hw2_data/weights.best.hdf5\n",
      "142/142 [==============================] - 128s - loss: 0.6474 - acc: 0.6800 - val_loss: 0.7042 - val_acc: 0.6580\n",
      "Epoch 26/35\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.6467 - acc: 0.6768Epoch 00025: val_acc did not improve\n",
      "142/142 [==============================] - 125s - loss: 0.6464 - acc: 0.6772 - val_loss: 0.7877 - val_acc: 0.6185\n",
      "Epoch 27/35\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.6405 - acc: 0.6791Epoch 00026: val_acc did not improve\n",
      "142/142 [==============================] - 127s - loss: 0.6402 - acc: 0.6791 - val_loss: 0.7342 - val_acc: 0.6510\n",
      "Epoch 28/35\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.6317 - acc: 0.6834Epoch 00027: val_acc did not improve\n",
      "142/142 [==============================] - 126s - loss: 0.6312 - acc: 0.6834 - val_loss: 0.7531 - val_acc: 0.6357\n",
      "Epoch 29/35\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.6288 - acc: 0.6831Epoch 00028: val_acc did not improve\n",
      "142/142 [==============================] - 127s - loss: 0.6285 - acc: 0.6833 - val_loss: 0.7887 - val_acc: 0.6211\n",
      "Epoch 30/35\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.6269 - acc: 0.6864Epoch 00029: val_acc did not improve\n",
      "142/142 [==============================] - 128s - loss: 0.6271 - acc: 0.6864 - val_loss: 0.7279 - val_acc: 0.6478\n",
      "Epoch 31/35\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.6262 - acc: 0.6886Epoch 00030: val_acc did not improve\n",
      "142/142 [==============================] - 128s - loss: 0.6258 - acc: 0.6889 - val_loss: 0.7192 - val_acc: 0.6425\n",
      "Epoch 32/35\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.6195 - acc: 0.6836Epoch 00031: val_acc did not improve\n",
      "142/142 [==============================] - 129s - loss: 0.6198 - acc: 0.6834 - val_loss: 0.7620 - val_acc: 0.6318\n",
      "Epoch 33/35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141/142 [============================>.] - ETA: 0s - loss: 0.6073 - acc: 0.6933Epoch 00032: val_acc did not improve\n",
      "142/142 [==============================] - 129s - loss: 0.6076 - acc: 0.6931 - val_loss: 0.7261 - val_acc: 0.6564\n",
      "Epoch 34/35\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.5984 - acc: 0.6923Epoch 00033: val_acc did not improve\n",
      "142/142 [==============================] - 129s - loss: 0.5980 - acc: 0.6923 - val_loss: 0.7300 - val_acc: 0.6371\n",
      "Epoch 35/35\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.6056 - acc: 0.6896Epoch 00034: val_acc improved from 0.65795 to 0.65902, saving model to ../Hw2_data/weights.best.hdf5\n",
      "142/142 [==============================] - 129s - loss: 0.6055 - acc: 0.6894 - val_loss: 0.7092 - val_acc: 0.6590\n",
      "Max val_acc: 65.90%\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "model = load_model('../Hw2_data/model.h5')\n",
    "\n",
    "total_val_acc += train(\n",
    "    model, \n",
    "    aug_mult=aug_mult, \n",
    "    epochs=35, # update when need to retest\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented training split:  18196\n",
      "Augmented test split:  2022\n",
      "Epoch 1/50\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.5890 - acc: 0.6958Epoch 00000: val_acc did not improve\n",
      "142/142 [==============================] - 71s - loss: 0.5883 - acc: 0.6961 - val_loss: 0.7327 - val_acc: 0.6419\n",
      "Epoch 2/50\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.5957 - acc: 0.6932Epoch 00001: val_acc improved from 0.65902 to 0.66329, saving model to ../Hw2_data/weights.best.hdf5\n",
      "142/142 [==============================] - 73s - loss: 0.5953 - acc: 0.6933 - val_loss: 0.6853 - val_acc: 0.6633\n",
      "Epoch 3/50\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.5770 - acc: 0.6984Epoch 00002: val_acc did not improve\n",
      "142/142 [==============================] - 92s - loss: 0.5772 - acc: 0.6985 - val_loss: 0.6925 - val_acc: 0.6592\n",
      "Epoch 4/50\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.5786 - acc: 0.6964Epoch 00003: val_acc did not improve\n",
      "142/142 [==============================] - 113s - loss: 0.5781 - acc: 0.6964 - val_loss: 0.6977 - val_acc: 0.6433\n",
      "Epoch 5/50\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.5713 - acc: 0.7000Epoch 00004: val_acc did not improve\n",
      "142/142 [==============================] - 121s - loss: 0.5708 - acc: 0.7001 - val_loss: 0.6714 - val_acc: 0.6543\n",
      "Epoch 6/50\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.5625 - acc: 0.7006Epoch 00005: val_acc did not improve\n",
      "142/142 [==============================] - 126s - loss: 0.5618 - acc: 0.7013 - val_loss: 0.6850 - val_acc: 0.6554\n",
      "Epoch 7/50\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.5610 - acc: 0.7027Epoch 00006: val_acc did not improve\n",
      "142/142 [==============================] - 128s - loss: 0.5606 - acc: 0.7029 - val_loss: 0.6926 - val_acc: 0.6531\n",
      "Epoch 8/50\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.5567 - acc: 0.7026Epoch 00007: val_acc did not improve\n",
      "142/142 [==============================] - 128s - loss: 0.5560 - acc: 0.7030 - val_loss: 0.7619 - val_acc: 0.6249\n",
      "Epoch 9/50\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.5593 - acc: 0.7025Epoch 00008: val_acc did not improve\n",
      "142/142 [==============================] - 129s - loss: 0.5592 - acc: 0.7028 - val_loss: 0.7057 - val_acc: 0.6505\n",
      "Epoch 10/50\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.5472 - acc: 0.7028Epoch 00009: val_acc did not improve\n",
      "142/142 [==============================] - 128s - loss: 0.5472 - acc: 0.7027 - val_loss: 0.7461 - val_acc: 0.6302\n",
      "Epoch 11/50\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.5369 - acc: 0.7104Epoch 00010: val_acc did not improve\n",
      "142/142 [==============================] - 129s - loss: 0.5370 - acc: 0.7102 - val_loss: 0.7071 - val_acc: 0.6345\n",
      "Epoch 12/50\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.5329 - acc: 0.7103Epoch 00011: val_acc did not improve\n",
      "142/142 [==============================] - 129s - loss: 0.5335 - acc: 0.7099 - val_loss: 0.7061 - val_acc: 0.6435\n",
      "Epoch 13/50\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.5262 - acc: 0.7124Epoch 00012: val_acc improved from 0.66329 to 0.66685, saving model to ../Hw2_data/weights.best.hdf5\n",
      "142/142 [==============================] - 129s - loss: 0.5265 - acc: 0.7122 - val_loss: 0.6460 - val_acc: 0.6668\n",
      "Epoch 14/50\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.5313 - acc: 0.7082Epoch 00013: val_acc did not improve\n",
      "142/142 [==============================] - 129s - loss: 0.5318 - acc: 0.7079 - val_loss: 0.8021 - val_acc: 0.6111\n",
      "Epoch 15/50\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.5261 - acc: 0.7133Epoch 00014: val_acc did not improve\n",
      "142/142 [==============================] - 129s - loss: 0.5258 - acc: 0.7136 - val_loss: 0.6745 - val_acc: 0.6638\n",
      "Epoch 16/50\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.5178 - acc: 0.7184Epoch 00015: val_acc did not improve\n",
      "142/142 [==============================] - 129s - loss: 0.5172 - acc: 0.7187 - val_loss: 0.7233 - val_acc: 0.6467\n",
      "Epoch 17/50\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.5132 - acc: 0.7208Epoch 00016: val_acc improved from 0.66685 to 0.73639, saving model to ../Hw2_data/weights.best.hdf5\n",
      "142/142 [==============================] - 128s - loss: 0.5126 - acc: 0.7213 - val_loss: 0.6489 - val_acc: 0.7364\n",
      "Epoch 18/50\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.5139 - acc: 0.7219Epoch 00017: val_acc did not improve\n",
      "142/142 [==============================] - 125s - loss: 0.5132 - acc: 0.7224 - val_loss: 0.6414 - val_acc: 0.7268\n",
      "Epoch 19/50\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.5079 - acc: 0.7211Epoch 00018: val_acc did not improve\n",
      "142/142 [==============================] - 124s - loss: 0.5073 - acc: 0.7212 - val_loss: 0.6706 - val_acc: 0.7300\n",
      "Epoch 20/50\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.4940 - acc: 0.7327Epoch 00019: val_acc did not improve\n",
      "142/142 [==============================] - 124s - loss: 0.4943 - acc: 0.7327 - val_loss: 0.6615 - val_acc: 0.7321\n",
      "Epoch 21/50\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.4959 - acc: 0.7305Epoch 00020: val_acc improved from 0.73639 to 0.75133, saving model to ../Hw2_data/weights.best.hdf5\n",
      "142/142 [==============================] - 125s - loss: 0.4956 - acc: 0.7307 - val_loss: 0.6341 - val_acc: 0.7513\n",
      "Epoch 22/50\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.4873 - acc: 0.7418Epoch 00021: val_acc did not improve\n",
      "142/142 [==============================] - 125s - loss: 0.4872 - acc: 0.7415 - val_loss: 0.6587 - val_acc: 0.7321\n",
      "Epoch 23/50\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.4864 - acc: 0.7355Epoch 00022: val_acc did not improve\n",
      "142/142 [==============================] - 124s - loss: 0.4859 - acc: 0.7353 - val_loss: 0.6459 - val_acc: 0.7300\n",
      "Epoch 24/50\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.4825 - acc: 0.7403Epoch 00023: val_acc did not improve\n",
      "142/142 [==============================] - 125s - loss: 0.4823 - acc: 0.7404 - val_loss: 0.6943 - val_acc: 0.7216\n",
      "Epoch 25/50\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.4791 - acc: 0.7437Epoch 00024: val_acc improved from 0.75133 to 0.75711, saving model to ../Hw2_data/weights.best.hdf5\n",
      "142/142 [==============================] - 125s - loss: 0.4795 - acc: 0.7434 - val_loss: 0.6460 - val_acc: 0.7571\n",
      "Epoch 26/50\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.4793 - acc: 0.7474Epoch 00025: val_acc improved from 0.75711 to 0.75720, saving model to ../Hw2_data/weights.best.hdf5\n",
      "142/142 [==============================] - 128s - loss: 0.4785 - acc: 0.7483 - val_loss: 0.6519 - val_acc: 0.7572\n",
      "Epoch 27/50\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.4793 - acc: 0.7431Epoch 00026: val_acc did not improve\n",
      "142/142 [==============================] - 128s - loss: 0.4787 - acc: 0.7435 - val_loss: 0.7790 - val_acc: 0.6809\n",
      "Epoch 28/50\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.4638 - acc: 0.7535Epoch 00027: val_acc improved from 0.75720 to 0.77001, saving model to ../Hw2_data/weights.best.hdf5\n",
      "142/142 [==============================] - 129s - loss: 0.4635 - acc: 0.7537 - val_loss: 0.6371 - val_acc: 0.7700\n",
      "Epoch 29/50\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.4688 - acc: 0.7497Epoch 00028: val_acc did not improve\n",
      "142/142 [==============================] - 128s - loss: 0.4690 - acc: 0.7500 - val_loss: 0.6920 - val_acc: 0.7396\n",
      "Epoch 30/50\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.4581 - acc: 0.7575Epoch 00029: val_acc did not improve\n",
      "142/142 [==============================] - 127s - loss: 0.4578 - acc: 0.7577 - val_loss: 0.6106 - val_acc: 0.7695\n",
      "Epoch 31/50\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.4599 - acc: 0.7603Epoch 00030: val_acc did not improve\n",
      "142/142 [==============================] - 126s - loss: 0.4597 - acc: 0.7605 - val_loss: 0.6315 - val_acc: 0.7647\n",
      "Epoch 32/50\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.4503 - acc: 0.7654Epoch 00031: val_acc did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142/142 [==============================] - 125s - loss: 0.4503 - acc: 0.7657 - val_loss: 0.6515 - val_acc: 0.7566\n",
      "Epoch 33/50\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.4508 - acc: 0.7658Epoch 00032: val_acc did not improve\n",
      "142/142 [==============================] - 124s - loss: 0.4512 - acc: 0.7658 - val_loss: 0.7169 - val_acc: 0.7257\n",
      "Epoch 34/50\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.4509 - acc: 0.7672Epoch 00033: val_acc did not improve\n",
      "142/142 [==============================] - 124s - loss: 0.4505 - acc: 0.7674 - val_loss: 0.6696 - val_acc: 0.7535\n",
      "Epoch 35/50\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.4403 - acc: 0.7726Epoch 00034: val_acc did not improve\n",
      "142/142 [==============================] - 123s - loss: 0.4401 - acc: 0.7729 - val_loss: 0.6918 - val_acc: 0.7402\n",
      "Epoch 36/50\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.4360 - acc: 0.7714Epoch 00035: val_acc did not improve\n",
      "142/142 [==============================] - 123s - loss: 0.4358 - acc: 0.7717 - val_loss: 0.6505 - val_acc: 0.7583\n",
      "Epoch 37/50\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.4367 - acc: 0.7729Epoch 00036: val_acc did not improve\n",
      "142/142 [==============================] - 124s - loss: 0.4367 - acc: 0.7730 - val_loss: 0.6491 - val_acc: 0.7668\n",
      "Epoch 38/50\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.4429 - acc: 0.7695Epoch 00037: val_acc did not improve\n",
      "142/142 [==============================] - 125s - loss: 0.4432 - acc: 0.7696 - val_loss: 0.7431 - val_acc: 0.7298\n",
      "Epoch 39/50\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.4325 - acc: 0.7786Epoch 00038: val_acc did not improve\n",
      "142/142 [==============================] - 125s - loss: 0.4328 - acc: 0.7785 - val_loss: 0.6530 - val_acc: 0.7647\n",
      "Epoch 40/50\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.4308 - acc: 0.7806Epoch 00039: val_acc did not improve\n",
      "142/142 [==============================] - 126s - loss: 0.4307 - acc: 0.7801 - val_loss: 0.6754 - val_acc: 0.7476\n",
      "Epoch 41/50\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.4297 - acc: 0.7769Epoch 00040: val_acc did not improve\n",
      "\n",
      "Epoch 00040: reducing learning rate to 4.999999873689376e-05.\n",
      "142/142 [==============================] - 127s - loss: 0.4297 - acc: 0.7768 - val_loss: 0.7173 - val_acc: 0.7232\n",
      "Epoch 42/50\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.4077 - acc: 0.7866Epoch 00041: val_acc improved from 0.77001 to 0.79048, saving model to ../Hw2_data/weights.best.hdf5\n",
      "142/142 [==============================] - 127s - loss: 0.4084 - acc: 0.7862 - val_loss: 0.6191 - val_acc: 0.7905\n",
      "Epoch 43/50\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.4066 - acc: 0.7871Epoch 00042: val_acc improved from 0.79048 to 0.80416, saving model to ../Hw2_data/weights.best.hdf5\n",
      "142/142 [==============================] - 126s - loss: 0.4062 - acc: 0.7873 - val_loss: 0.6021 - val_acc: 0.8042\n",
      "Epoch 44/50\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.3993 - acc: 0.7935Epoch 00043: val_acc did not improve\n",
      "142/142 [==============================] - 125s - loss: 0.3998 - acc: 0.7935 - val_loss: 0.6027 - val_acc: 0.7967\n",
      "Epoch 45/50\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.3900 - acc: 0.7947Epoch 00044: val_acc improved from 0.80416 to 0.80470, saving model to ../Hw2_data/weights.best.hdf5\n",
      "142/142 [==============================] - 127s - loss: 0.3897 - acc: 0.7948 - val_loss: 0.5962 - val_acc: 0.8047\n",
      "Epoch 46/50\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.3928 - acc: 0.7955Epoch 00045: val_acc improved from 0.80470 to 0.81323, saving model to ../Hw2_data/weights.best.hdf5\n",
      "142/142 [==============================] - 128s - loss: 0.3928 - acc: 0.7951 - val_loss: 0.5861 - val_acc: 0.8132\n",
      "Epoch 47/50\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.3879 - acc: 0.7983Epoch 00046: val_acc did not improve\n",
      "142/142 [==============================] - 126s - loss: 0.3879 - acc: 0.7983 - val_loss: 0.6093 - val_acc: 0.7914\n",
      "Epoch 48/50\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.3959 - acc: 0.7961Epoch 00047: val_acc did not improve\n",
      "142/142 [==============================] - 125s - loss: 0.3959 - acc: 0.7964 - val_loss: 0.5964 - val_acc: 0.8004\n",
      "Epoch 49/50\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.3896 - acc: 0.7959Epoch 00048: val_acc did not improve\n",
      "142/142 [==============================] - 124s - loss: 0.3892 - acc: 0.7964 - val_loss: 0.6091 - val_acc: 0.8091\n",
      "Epoch 50/50\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.3868 - acc: 0.7964Epoch 00049: val_acc did not improve\n",
      "142/142 [==============================] - 123s - loss: 0.3874 - acc: 0.7960 - val_loss: 0.6157 - val_acc: 0.7960\n",
      "Max val_acc: 81.32%\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "model = load_model('../Hw2_data/model.h5')\n",
    "\n",
    "total_val_acc += train(\n",
    "    model, \n",
    "    aug_mult=aug_mult, \n",
    "    epochs=50, # update when need to retest\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented training split:  18196\n",
      "Augmented test split:  2022\n",
      "Epoch 1/30\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.3873 - acc: 0.7986Epoch 00000: val_acc did not improve\n",
      "142/142 [==============================] - 74s - loss: 0.3878 - acc: 0.7984 - val_loss: 0.6080 - val_acc: 0.8047\n",
      "Epoch 2/30\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.3783 - acc: 0.7995Epoch 00001: val_acc did not improve\n",
      "142/142 [==============================] - 101s - loss: 0.3779 - acc: 0.7995 - val_loss: 0.6090 - val_acc: 0.8074\n",
      "Epoch 3/30\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.3849 - acc: 0.7994Epoch 00002: val_acc did not improve\n",
      "142/142 [==============================] - 115s - loss: 0.3849 - acc: 0.7991 - val_loss: 0.6104 - val_acc: 0.8004\n",
      "Epoch 4/30\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.3800 - acc: 0.8066Epoch 00003: val_acc did not improve\n",
      "142/142 [==============================] - 123s - loss: 0.3798 - acc: 0.8065 - val_loss: 0.5922 - val_acc: 0.8074\n",
      "Epoch 5/30\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.3772 - acc: 0.8056Epoch 00004: val_acc did not improve\n",
      "142/142 [==============================] - 126s - loss: 0.3776 - acc: 0.8050 - val_loss: 0.6172 - val_acc: 0.8015\n",
      "Epoch 6/30\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.3739 - acc: 0.8070Epoch 00005: val_acc did not improve\n",
      "142/142 [==============================] - 127s - loss: 0.3742 - acc: 0.8069 - val_loss: 0.5974 - val_acc: 0.8084\n",
      "Epoch 7/30\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.3781 - acc: 0.8050Epoch 00006: val_acc did not improve\n",
      "142/142 [==============================] - 127s - loss: 0.3779 - acc: 0.8052 - val_loss: 0.6413 - val_acc: 0.7876\n",
      "Epoch 8/30\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.3684 - acc: 0.8100Epoch 00007: val_acc did not improve\n",
      "142/142 [==============================] - 127s - loss: 0.3683 - acc: 0.8104 - val_loss: 0.6070 - val_acc: 0.8058\n",
      "Epoch 9/30\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.3658 - acc: 0.8174Epoch 00008: val_acc improved from 0.81323 to 0.81783, saving model to ../Hw2_data/weights.best.hdf5\n",
      "142/142 [==============================] - 129s - loss: 0.3654 - acc: 0.8177 - val_loss: 0.6127 - val_acc: 0.8178\n",
      "Epoch 10/30\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.3758 - acc: 0.8101Epoch 00009: val_acc did not improve\n",
      "142/142 [==============================] - 124s - loss: 0.3757 - acc: 0.8100 - val_loss: 0.6263 - val_acc: 0.7938\n",
      "Epoch 11/30\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.3578 - acc: 0.8186Epoch 00010: val_acc did not improve\n",
      "142/142 [==============================] - 125s - loss: 0.3579 - acc: 0.8187 - val_loss: 0.6391 - val_acc: 0.7965\n",
      "Epoch 12/30\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.3666 - acc: 0.8250Epoch 00011: val_acc did not improve\n",
      "142/142 [==============================] - 124s - loss: 0.3669 - acc: 0.8250 - val_loss: 0.6237 - val_acc: 0.8031\n",
      "Epoch 13/30\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.3588 - acc: 0.8263Epoch 00012: val_acc did not improve\n",
      "142/142 [==============================] - 123s - loss: 0.3594 - acc: 0.8259 - val_loss: 0.5891 - val_acc: 0.8175\n",
      "Epoch 14/30\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.3574 - acc: 0.8279Epoch 00013: val_acc did not improve\n",
      "142/142 [==============================] - 124s - loss: 0.3574 - acc: 0.8281 - val_loss: 0.6289 - val_acc: 0.8091\n",
      "Epoch 15/30\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.3593 - acc: 0.8261Epoch 00014: val_acc did not improve\n",
      "142/142 [==============================] - 124s - loss: 0.3592 - acc: 0.8259 - val_loss: 0.6198 - val_acc: 0.8080\n",
      "Epoch 16/30\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.3459 - acc: 0.8334Epoch 00015: val_acc did not improve\n",
      "142/142 [==============================] - 125s - loss: 0.3467 - acc: 0.8331 - val_loss: 0.6025 - val_acc: 0.8063\n",
      "Epoch 17/30\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.3490 - acc: 0.8351Epoch 00016: val_acc did not improve\n",
      "142/142 [==============================] - 126s - loss: 0.3489 - acc: 0.8349 - val_loss: 0.6202 - val_acc: 0.8091\n",
      "Epoch 18/30\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.3485 - acc: 0.8359Epoch 00017: val_acc did not improve\n",
      "142/142 [==============================] - 126s - loss: 0.3487 - acc: 0.8360 - val_loss: 0.6002 - val_acc: 0.8173\n",
      "Epoch 19/30\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.3359 - acc: 0.8444Epoch 00018: val_acc did not improve\n",
      "142/142 [==============================] - 127s - loss: 0.3361 - acc: 0.8443 - val_loss: 0.6035 - val_acc: 0.8074\n",
      "Epoch 20/30\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.3406 - acc: 0.8392Epoch 00019: val_acc did not improve\n",
      "142/142 [==============================] - 128s - loss: 0.3400 - acc: 0.8394 - val_loss: 0.6168 - val_acc: 0.8074\n",
      "Epoch 21/30\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.3425 - acc: 0.8409Epoch 00020: val_acc did not improve\n",
      "142/142 [==============================] - 128s - loss: 0.3425 - acc: 0.8411 - val_loss: 0.6326 - val_acc: 0.8100\n",
      "Epoch 22/30\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.3325 - acc: 0.8453Epoch 00021: val_acc did not improve\n",
      "142/142 [==============================] - 127s - loss: 0.3334 - acc: 0.8451 - val_loss: 0.6194 - val_acc: 0.8175\n",
      "Epoch 23/30\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.3370 - acc: 0.8464Epoch 00022: val_acc did not improve\n",
      "142/142 [==============================] - 126s - loss: 0.3370 - acc: 0.8465 - val_loss: 0.6142 - val_acc: 0.8074\n",
      "Epoch 24/30\n",
      "141/142 [============================>.] - ETA: 0s - loss: 0.3337 - acc: 0.8473Epoch 00023: val_acc improved from 0.81783 to 0.82657, saving model to ../Hw2_data/weights.best.hdf5\n",
      "142/142 [==============================] - 125s - loss: 0.3336 - acc: 0.8474 - val_loss: 0.5851 - val_acc: 0.8266\n",
      "Epoch 25/30\n",
      "102/142 [====================>.........] - ETA: 33s - loss: 0.3247 - acc: 0.8542"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "model = load_model('../Hw2_data/model.h5')\n",
    "\n",
    "total_val_acc += train(\n",
    "    model, \n",
    "    aug_mult=aug_mult, \n",
    "    epochs=30, # update when need to retest\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load model\n",
    "model = load_model('../Hw2_data/model.h5')\n",
    "\n",
    "total_val_acc += train(\n",
    "    model, \n",
    "    aug_mult=aug_mult, \n",
    "    epochs=60, # update when need to retest\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('../Hw2_data/weights.best.hdf5')\n",
    "\n",
    "# load test data\n",
    "test_images = pd.read_csv('../Hw2_data/test_data.csv', header=None).values.astype('float32')\n",
    "\n",
    "sample_labels = pd.read_csv('../Hw2_data/sample-submission.csv')\n",
    "sample_labels.set_index('Id')\n",
    "sample_labels = sample_labels['Category'].values.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = whiten(test_images)\n",
    "print(test_images.shape, sample_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = result.reshape(-1,1,48,48)\n",
    "result = result.reshape(-1,1,48,48)\n",
    "\n",
    "print(X_test.shape, test_images.shape)\n",
    "\n",
    "i = 3\n",
    "plt.subplot(1, 2, 1)\n",
    "orig_image = test_images.reshape(test_images.shape[0], 48, 48)[i]\n",
    "plt.title('Original')\n",
    "plt.imshow(orig_image, cmap=plt.get_cmap('gray'))\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "test_image = result.reshape(result.shape[0], 48, 48)[i]\n",
    "plt.title('ZCA Whitening')\n",
    "plt.imshow(test_image, cmap=plt.get_cmap('gray'))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "X_test /= 255\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../Hw2_data/predictions-deep.csv', 'w', newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(('Id', 'Category'))\n",
    "    for i in range(0, test_images.shape[0]):\n",
    "        writer.writerow((i, np.argmax(predictions[i])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
